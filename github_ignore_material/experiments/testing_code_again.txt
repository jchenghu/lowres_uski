

# # # # # # #
    Test SENZA USKI:

    # Lascio uz-en_4000 vediamo se torna...


    python3.7 train.py \
            --selected_model transformer --N 3 \
            --dropout 0.15 --seed 1234  --num_epochs 300 \
            --max_len 200  --min_len 2 \
            --d_model 128 \
            --sched_type noam --warmup_steps 400 \
            --batch_size 2048 --num_accum 2 \
            --eval_beam_size 4 \
            --eval_batch_size 32 \
            --save_path ./github_ignore_material/saves/ \
            --language uz-en_4000 --num_gpus 1 \
            --ddp_sync_port 12139 &> NORMAL_output.txt &



Starting: __main__
seed: 1234
./bpe_data/qed_uz_en/4000_train.uz read 3689 sentences
./bpe_data/qed_uz_en/4000_train.en read 3689 sentences
Dictionary generated. Shared_word2idx_dict len: 4356
Word examples: [<PAD>, <SOS>, <EOS>]
./bpe_data/qed_uz_en/4000_val.uz read 99 sentences
./bpe_data/qed_uz_en/4000_val.en read 99 sentences
------------------- Train Set before preproc -------------------------
0) --------------------------------------------
Src : ['men', 'bu', 'yerda', "bo'ldim", '.']
Trg : ['i', 'was', 'here', '.']
1) --------------------------------------------
Src : ['j@@', 'o@@', 'e', 'k@@', 'ra@@', 'us', 'aytadi', ',']
Trg : ['j@@', 'o@@', 'e', 'k@@', 'ra@@', 'us', 'says', ',']
------------------- Train Set before preproc -------------------------
0) --------------------------------------------
Src : tensor([   1,  153,  147, 1306, 1798,   21,    2])
Trg : tensor([   1,  324, 3068, 3155,   21,    2])
1) --------------------------------------------
Src : tensor([   1,  325,  214,  483,  139,  941, 1788, 2012,   10,    2])
Trg : tensor([   1,  325,  214,  483,  139,  941, 1788, 3377,   10,    2])
Len: 96
./bpe_data/qed_uz_en/4000_test.uz read 199 sentences
./bpe_data/qed_uz_en/4000_test.en read 199 sentences
------------------- Train Set before preproc -------------------------
0) --------------------------------------------
Src : ['chunki', 'dunyo', 'muammo@@', 'lari', 'inson@@', 'larning', 'o@@', 'ila@@', 'viy', 'mer@@', 'o@@', 'si', "bo'lishi", 'kerak', 'emas', '.']
Trg : ['because', 'the', 'world', "'s", 'problems', 'sh@@', 'ouldn', "'t", 'be', 'the', 'human', 'family', "'s", 'he@@', 'ir@@', 'lo@@', 'om', '.']
1) --------------------------------------------
Src : ['(', 'qarsaklar', ')']
Trg : ['(', 'applause', ')']
------------------- Train Set before preproc -------------------------
0) --------------------------------------------
Src : tensor([   1,  346, 1482, 2090,  299,  691,  787,  214, 2401, 1438, 2680,  214,
         753, 1292,  162,  226,   21,    2])
Trg : ['because', 'the', 'world', "'s", 'problems', 'sh@@', 'ouldn', "'t", 'be', 'the', 'human', 'family', "'s", 'he@@', 'ir@@', 'lo@@', 'om', '.']
1) --------------------------------------------
Src : tensor([  1, 105,   3, 107,   2])
Trg : ['(', 'applause', ')']
Len: 193
./bpe_data/qed_uz_en/4000_train.uz read 3689 sentences
./bpe_data/qed_uz_en/4000_train.en read 3689 sentences
------------------- Train Set before preproc -------------------------
0) --------------------------------------------
Src : ['ka', ':']
Trg : ['ca', ':']
1) --------------------------------------------
Src : ['bizning', "ta'lim", 'soha@@', 'miz', ',', 'a@@', 'ka@@', 'demic', 'qobiliyat@@', 'ni', 'ba@@', 'sh@@', 'or@@', 'at', 'qiladi', '.']
Trg : ['g@@', 'ro@@', 'wn', 'men', 'and', 'women', 'wri@@', 'thing', 'un@@', 'contro@@', 'l@@', 'lab@@', 'ly', ',', 'off', 'the', 'be@@', 'at', '.', '(', 'laughter', ')', 'wa@@', 'iting', 'until', 'it', 'ends', 'so', 'they', 'can', 'go', 'home', 'and', 'write', 'a', 'pa@@', 'per', 'about', 'it', '.', '(', 'laughter', ')']
------------------- Train Set before preproc -------------------------
0) --------------------------------------------
Src : [1, 235, 5, 2]
Trg : [1, 617, 5, 2]
1) --------------------------------------------
Src : [1, 844, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 2]
Trg : [1, 520, 734, 2821, 153, 243, 2945, 2946, 2947, 616, 2948, 699, 1088, 2682, 10, 2949, 2950, 1822, 19, 21, 105, 3916, 107, 3131, 2952, 2953, 2092, 2954, 1116, 2917, 2853, 2652, 2955, 243, 2956, 136, 65, 2363, 2957, 2092, 21, 105, 3916, 107, 2]
Len: 3539
Using -  1  processes / GPUs!
Requested num GPUs: 1
GPU: 0] Process 0 working...
Creating pretraining - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
<class 'model.transformer.TransformerMT'>
The model has 2113284 trainable parameters
Start NMT Training
Loading: no checkpoint found in ./github_ignore_material/saves/
How many batches: 44
Epoch: 0 Train loss: 7.0531 acc: 0.02 | lr: 0.0002 | Elapsed: 0.0551 min
Epoch: 1 Train loss: 5.6699 acc: 0.0465 | lr: 0.0005 | Elapsed: 0.0914 min
Epoch: 2 Train loss: 5.4626 acc: 0.0558 | lr: 0.0007 | Elapsed: 0.127 min
Epoch: 3 Train loss: 5.4092 acc: 0.0587 | lr: 0.001 | Elapsed: 0.1633 min
Epoch: 4 Train loss: 5.3708 acc: 0.0621 | lr: 0.0012 | Elapsed: 0.1989 min
Epoch: 5 Train loss: 5.3103 acc: 0.0763 | lr: 0.0015 | Elapsed: 0.235 min
Epoch: 6 Train loss: 5.2619 acc: 0.0891 | lr: 0.0017 | Elapsed: 0.2709 min
Epoch: 7 Train loss: 5.2039 acc: 0.0905 | lr: 0.0019 | Elapsed: 0.3069 min
Epoch: 8 Train loss: 5.1199 acc: 0.0982 | lr: 0.0022 | Elapsed: 0.3427 min
Epoch: 9 Train loss: 5.0081 acc: 0.1088 | lr: 0.0024 | Elapsed: 0.3788 min
Epoch: 10 Train loss: 4.8836 acc: 0.1144 | lr: 0.0027 | Elapsed: 0.4147 min
Epoch: 11 Train loss: 4.7395 acc: 0.1224 | lr: 0.0029 | Elapsed: 0.4507 min
Epoch: 12 Train loss: 4.6293 acc: 0.1302 | lr: 0.0032 | Elapsed: 0.4867 min
Epoch: 13 Train loss: 4.5127 acc: 0.1434 | lr: 0.0034 | Elapsed: 0.5224 min
Epoch: 14 Train loss: 4.438 acc: 0.1506 | lr: 0.0036 | Elapsed: 0.5587 min
Epoch: 15 Train loss: 4.3633 acc: 0.1564 | lr: 0.0039 | Elapsed: 0.5936 min
Epoch: 16 Train loss: 4.3091 acc: 0.1624 | lr: 0.0041 | Elapsed: 0.6273 min
Epoch: 17 Train loss: 4.2175 acc: 0.1672 | lr: 0.0044 | Elapsed: 0.6612 min
Epoch: 18 Train loss: 4.0956 acc: 0.1813 | lr: 0.0043 | Elapsed: 0.6965 min
Epoch: 19 Train loss: 4.0007 acc: 0.1922 | lr: 0.0042 | Elapsed: 0.7322 min
Epoch: 20 Train loss: 3.9066 acc: 0.2008 | lr: 0.0041 | Elapsed: 0.7681 min
Epoch: 21 Train loss: 3.7871 acc: 0.215 | lr: 0.004 | Elapsed: 0.8043 min
Epoch: 22 Train loss: 3.7054 acc: 0.2246 | lr: 0.0039 | Elapsed: 0.8406 min
Epoch: 23 Train loss: 3.6141 acc: 0.2347 | lr: 0.0038 | Elapsed: 0.8773 min
Epoch: 24 Train loss: 3.5611 acc: 0.2386 | lr: 0.0038 | Elapsed: 0.9137 min
Epoch: 25 Train loss: 3.4898 acc: 0.2486 | lr: 0.0037 | Elapsed: 0.95 min
Epoch: 26 Train loss: 3.423 acc: 0.2552 | lr: 0.0036 | Elapsed: 0.9868 min
Epoch: 27 Train loss: 3.3724 acc: 0.2604 | lr: 0.0036 | Elapsed: 1.0237 min
Epoch: 28 Train loss: 3.3002 acc: 0.2727 | lr: 0.0035 | Elapsed: 1.0596 min
Epoch: 29 Train loss: 3.2355 acc: 0.2807 | lr: 0.0034 | Elapsed: 1.096 min
Epoch: 30 Train loss: 3.1751 acc: 0.2907 | lr: 0.0034 | Elapsed: 1.1327 min
Epoch: 31 Train loss: 3.127 acc: 0.2951 | lr: 0.0033 | Elapsed: 1.1685 min
Epoch: 32 Train loss: 3.0686 acc: 0.3046 | lr: 0.0033 | Elapsed: 1.2047 min
Epoch: 33 Train loss: 3.0257 acc: 0.3105 | lr: 0.0032 | Elapsed: 1.2413 min
Epoch: 34 Train loss: 2.9834 acc: 0.3154 | lr: 0.0032 | Elapsed: 1.2781 min
Epoch: 35 Train loss: 2.9429 acc: 0.3218 | lr: 0.0031 | Elapsed: 1.3138 min
Epoch: 36 Train loss: 2.8933 acc: 0.3308 | lr: 0.0031 | Elapsed: 1.3512 min
Epoch: 37 Train loss: 2.855 acc: 0.3352 | lr: 0.0031 | Elapsed: 1.387 min
Epoch: 38 Train loss: 2.8164 acc: 0.342 | lr: 0.003 | Elapsed: 1.4239 min
Epoch: 39 Train loss: 2.7826 acc: 0.3481 | lr: 0.003 | Elapsed: 1.4609 min
Epoch: 40 Train loss: 2.7419 acc: 0.3562 | lr: 0.0029 | Elapsed: 1.4978 min
Epoch: 41 Train loss: 2.709 acc: 0.3607 | lr: 0.0029 | Elapsed: 1.5349 min
Epoch: 42 Train loss: 2.6769 acc: 0.3649 | lr: 0.0029 | Elapsed: 1.5719 min
Epoch: 43 Train loss: 2.645 acc: 0.3719 | lr: 0.0028 | Elapsed: 1.6084 min
Epoch: 44 Train loss: 2.6132 acc: 0.3756 | lr: 0.0028 | Elapsed: 1.6449 min
Epoch: 45 Train loss: 2.5765 acc: 0.3811 | lr: 0.0028 | Elapsed: 1.6819 min
Epoch: 46 Train loss: 2.544 acc: 0.3887 | lr: 0.0027 | Elapsed: 1.7185 min
Epoch: 47 Train loss: 2.5273 acc: 0.3908 | lr: 0.0027 | Elapsed: 1.7552 min
Epoch: 48 Train loss: 2.4935 acc: 0.3976 | lr: 0.0027 | Elapsed: 1.7923 min
Epoch: 49 Train loss: 2.4606 acc: 0.404 | lr: 0.0027 | Elapsed: 1.8287 min
Epoch: 50 Train loss: 2.4345 acc: 0.4078 | lr: 0.0026 | Elapsed: 1.8652 min
Epoch: 51 Train loss: 2.4238 acc: 0.4087 | lr: 0.0026 | Elapsed: 1.9022 min
Epoch: 52 Train loss: 2.3947 acc: 0.4168 | lr: 0.0026 | Elapsed: 1.9386 min
Epoch: 53 Train loss: 2.3719 acc: 0.4207 | lr: 0.0026 | Elapsed: 1.9757 min
Epoch: 54 Train loss: 2.348 acc: 0.427 | lr: 0.0025 | Elapsed: 2.0126 min
Epoch: 55 Train loss: 2.3334 acc: 0.4269 | lr: 0.0025 | Elapsed: 2.0498 min
Epoch: 56 Train loss: 2.3002 acc: 0.4333 | lr: 0.0025 | Elapsed: 2.0872 min
Epoch: 57 Train loss: 2.2836 acc: 0.4377 | lr: 0.0025 | Elapsed: 2.1245 min
Epoch: 58 Train loss: 2.253 acc: 0.4433 | lr: 0.0025 | Elapsed: 2.1616 min
Epoch: 59 Train loss: 2.2423 acc: 0.445 | lr: 0.0024 | Elapsed: 2.1984 min
Epoch: 60 Train loss: 2.2062 acc: 0.453 | lr: 0.0024 | Elapsed: 2.2361 min
Epoch: 61 Train loss: 2.195 acc: 0.4544 | lr: 0.0024 | Elapsed: 2.2735 min
Epoch: 62 Train loss: 2.1757 acc: 0.4587 | lr: 0.0024 | Elapsed: 2.311 min
Epoch: 63 Train loss: 2.152 acc: 0.4654 | lr: 0.0024 | Elapsed: 2.3486 min
Epoch: 64 Train loss: 2.1388 acc: 0.4672 | lr: 0.0023 | Elapsed: 2.384 min
Epoch: 65 Train loss: 2.1126 acc: 0.4714 | lr: 0.0023 | Elapsed: 2.4212 min
Epoch: 66 Train loss: 2.0988 acc: 0.4758 | lr: 0.0023 | Elapsed: 2.4581 min
Epoch: 67 Train loss: 2.0816 acc: 0.4773 | lr: 0.0023 | Elapsed: 2.495 min
Epoch: 68 Train loss: 2.0627 acc: 0.482 | lr: 0.0023 | Elapsed: 2.5323 min
Epoch: 69 Train loss: 2.0537 acc: 0.4852 | lr: 0.0023 | Elapsed: 2.5694 min
Epoch: 70 Train loss: 2.0328 acc: 0.4889 | lr: 0.0022 | Elapsed: 2.6064 min
Epoch: 71 Train loss: 2.0201 acc: 0.4887 | lr: 0.0022 | Elapsed: 2.6434 min
Epoch: 72 Train loss: 2.0064 acc: 0.4931 | lr: 0.0022 | Elapsed: 2.6805 min
Epoch: 73 Train loss: 1.9851 acc: 0.4987 | lr: 0.0022 | Elapsed: 2.7181 min
Epoch: 74 Train loss: 1.9729 acc: 0.5012 | lr: 0.0022 | Elapsed: 2.7546 min
Epoch: 75 Train loss: 1.9496 acc: 0.5066 | lr: 0.0022 | Elapsed: 2.7907 min
Epoch: 76 Train loss: 1.9356 acc: 0.5105 | lr: 0.0021 | Elapsed: 2.8278 min
Epoch: 77 Train loss: 1.9189 acc: 0.5132 | lr: 0.0021 | Elapsed: 2.864 min
Epoch: 78 Train loss: 1.9109 acc: 0.5138 | lr: 0.0021 | Elapsed: 2.9014 min
Epoch: 79 Train loss: 1.9089 acc: 0.5156 | lr: 0.0021 | Elapsed: 2.9376 min
Epoch: 80 Train loss: 1.8827 acc: 0.5213 | lr: 0.0021 | Elapsed: 2.9747 min
Epoch: 81 Train loss: 1.877 acc: 0.5221 | lr: 0.0021 | Elapsed: 3.0116 min
Epoch: 82 Train loss: 1.86 acc: 0.5252 | lr: 0.0021 | Elapsed: 3.0484 min
Epoch: 83 Train loss: 1.8502 acc: 0.5283 | lr: 0.0021 | Elapsed: 3.0857 min
Epoch: 84 Train loss: 1.8296 acc: 0.5329 | lr: 0.002 | Elapsed: 3.123 min
Epoch: 85 Train loss: 1.8245 acc: 0.5339 | lr: 0.002 | Elapsed: 3.1604 min
Epoch: 86 Train loss: 1.8059 acc: 0.538 | lr: 0.002 | Elapsed: 3.1974 min
Epoch: 87 Train loss: 1.8036 acc: 0.5368 | lr: 0.002 | Elapsed: 3.234 min
Epoch: 88 Train loss: 1.7865 acc: 0.5422 | lr: 0.002 | Elapsed: 3.2706 min
Epoch: 89 Train loss: 1.7719 acc: 0.5467 | lr: 0.002 | Elapsed: 3.3064 min
Epoch: 90 Train loss: 1.7581 acc: 0.5499 | lr: 0.002 | Elapsed: 3.3436 min
Epoch: 91 Train loss: 1.7531 acc: 0.5511 | lr: 0.002 | Elapsed: 3.3795 min
Epoch: 92 Train loss: 1.7423 acc: 0.5521 | lr: 0.002 | Elapsed: 3.4159 min
Epoch: 93 Train loss: 1.7286 acc: 0.5551 | lr: 0.0019 | Elapsed: 3.4512 min
Epoch: 94 Train loss: 1.7224 acc: 0.5566 | lr: 0.0019 | Elapsed: 3.486 min
Epoch: 95 Train loss: 1.7103 acc: 0.5594 | lr: 0.0019 | Elapsed: 3.5232 min
Epoch: 96 Train loss: 1.6958 acc: 0.5635 | lr: 0.0019 | Elapsed: 3.5608 min
Epoch: 97 Train loss: 1.6803 acc: 0.5672 | lr: 0.0019 | Elapsed: 3.598 min
Epoch: 98 Train loss: 1.6802 acc: 0.5681 | lr: 0.0019 | Elapsed: 3.6346 min
Epoch: 99 Train loss: 1.6665 acc: 0.5696 | lr: 0.0019 | Elapsed: 3.6718 min
Epoch: 100 Train loss: 1.648 acc: 0.5739 | lr: 0.0019 | Elapsed: 3.7089 min
Epoch: 101 Train loss: 1.649 acc: 0.5738 | lr: 0.0019 | Elapsed: 3.7463 min
Epoch: 102 Train loss: 1.6363 acc: 0.5771 | lr: 0.0019 | Elapsed: 3.7841 min
Epoch: 103 Train loss: 1.6209 acc: 0.5807 | lr: 0.0018 | Elapsed: 3.8215 min
Epoch: 104 Train loss: 1.6203 acc: 0.5803 | lr: 0.0018 | Elapsed: 3.8587 min
Epoch: 105 Train loss: 1.6105 acc: 0.5826 | lr: 0.0018 | Elapsed: 3.8955 min
Epoch: 106 Train loss: 1.6068 acc: 0.5828 | lr: 0.0018 | Elapsed: 3.9327 min
Epoch: 107 Train loss: 1.5808 acc: 0.5912 | lr: 0.0018 | Elapsed: 3.9706 min
Epoch: 108 Train loss: 1.5852 acc: 0.5893 | lr: 0.0018 | Elapsed: 4.0078 min
Epoch: 109 Train loss: 1.5721 acc: 0.5935 | lr: 0.0018 | Elapsed: 4.0454 min
Epoch: 110 Train loss: 1.5694 acc: 0.5962 | lr: 0.0018 | Elapsed: 4.083 min
Epoch: 111 Train loss: 1.5649 acc: 0.5939 | lr: 0.0018 | Elapsed: 4.1178 min
Epoch: 112 Train loss: 1.5521 acc: 0.5973 | lr: 0.0018 | Elapsed: 4.1551 min
Epoch: 113 Train loss: 1.5429 acc: 0.6007 | lr: 0.0018 | Elapsed: 4.1923 min
Epoch: 114 Train loss: 1.5328 acc: 0.6046 | lr: 0.0018 | Elapsed: 4.2297 min
Epoch: 115 Train loss: 1.5327 acc: 0.6028 | lr: 0.0017 | Elapsed: 4.2674 min
Epoch: 116 Train loss: 1.5205 acc: 0.6062 | lr: 0.0017 | Elapsed: 4.3045 min
Epoch: 117 Train loss: 1.5143 acc: 0.6065 | lr: 0.0017 | Elapsed: 4.3415 min
Epoch: 118 Train loss: 1.5079 acc: 0.611 | lr: 0.0017 | Elapsed: 4.3746 min
Epoch: 119 Train loss: 1.5019 acc: 0.6092 | lr: 0.0017 | Elapsed: 4.4076 min
Epoch: 120 Train loss: 1.4968 acc: 0.6117 | lr: 0.0017 | Elapsed: 4.4451 min
Epoch: 121 Train loss: 1.494 acc: 0.6131 | lr: 0.0017 | Elapsed: 4.4824 min
Epoch: 122 Train loss: 1.4872 acc: 0.6134 | lr: 0.0017 | Elapsed: 4.52 min
Epoch: 123 Train loss: 1.4756 acc: 0.6166 | lr: 0.0017 | Elapsed: 4.5577 min
Epoch: 124 Train loss: 1.4656 acc: 0.6189 | lr: 0.0017 | Elapsed: 4.5957 min
Epoch: 125 Train loss: 1.456 acc: 0.6225 | lr: 0.0017 | Elapsed: 4.6341 min
Epoch: 126 Train loss: 1.4631 acc: 0.6207 | lr: 0.0017 | Elapsed: 4.6723 min
Epoch: 127 Train loss: 1.4443 acc: 0.6263 | lr: 0.0017 | Elapsed: 4.7099 min
Epoch: 128 Train loss: 1.4465 acc: 0.6255 | lr: 0.0017 | Elapsed: 4.7472 min
Epoch: 129 Train loss: 1.4396 acc: 0.6273 | lr: 0.0017 | Elapsed: 4.7848 min
Epoch: 130 Train loss: 1.4367 acc: 0.6272 | lr: 0.0016 | Elapsed: 4.8225 min
Epoch: 131 Train loss: 1.4251 acc: 0.6304 | lr: 0.0016 | Elapsed: 4.8599 min
Epoch: 132 Train loss: 1.4205 acc: 0.6313 | lr: 0.0016 | Elapsed: 4.8975 min
Epoch: 133 Train loss: 1.4107 acc: 0.6357 | lr: 0.0016 | Elapsed: 4.9352 min
Epoch: 134 Train loss: 1.4072 acc: 0.635 | lr: 0.0016 | Elapsed: 4.9731 min
Epoch: 135 Train loss: 1.4056 acc: 0.6352 | lr: 0.0016 | Elapsed: 5.0102 min
Epoch: 136 Train loss: 1.3877 acc: 0.6413 | lr: 0.0016 | Elapsed: 5.0479 min
Epoch: 137 Train loss: 1.3927 acc: 0.6392 | lr: 0.0016 | Elapsed: 5.0857 min
Epoch: 138 Train loss: 1.3835 acc: 0.6424 | lr: 0.0016 | Elapsed: 5.1235 min
Epoch: 139 Train loss: 1.3783 acc: 0.6435 | lr: 0.0016 | Elapsed: 5.1607 min
Epoch: 140 Train loss: 1.3711 acc: 0.6445 | lr: 0.0016 | Elapsed: 5.1982 min
Epoch: 141 Train loss: 1.3715 acc: 0.6447 | lr: 0.0016 | Elapsed: 5.2357 min
Epoch: 142 Train loss: 1.368 acc: 0.6459 | lr: 0.0016 | Elapsed: 5.2738 min
Epoch: 143 Train loss: 1.3552 acc: 0.6498 | lr: 0.0016 | Elapsed: 5.3111 min
Epoch: 144 Train loss: 1.3474 acc: 0.6522 | lr: 0.0016 | Elapsed: 5.3486 min
Epoch: 145 Train loss: 1.3405 acc: 0.6538 | lr: 0.0016 | Elapsed: 5.3863 min
Epoch: 146 Train loss: 1.3356 acc: 0.6551 | lr: 0.0016 | Elapsed: 5.4241 min
Epoch: 147 Train loss: 1.3309 acc: 0.654 | lr: 0.0015 | Elapsed: 5.4614 min
Epoch: 148 Train loss: 1.3292 acc: 0.6567 | lr: 0.0015 | Elapsed: 5.4987 min
Epoch: 149 Train loss: 1.3299 acc: 0.6548 | lr: 0.0015 | Elapsed: 5.5366 min
Epoch: 150 Train loss: 1.3232 acc: 0.6602 | lr: 0.0015 | Elapsed: 5.5744 min
Epoch: 151 Train loss: 1.3148 acc: 0.6613 | lr: 0.0015 | Elapsed: 5.6118 min
Epoch: 152 Train loss: 1.3168 acc: 0.6598 | lr: 0.0015 | Elapsed: 5.6497 min
Epoch: 153 Train loss: 1.3084 acc: 0.6592 | lr: 0.0015 | Elapsed: 5.687 min
Epoch: 154 Train loss: 1.3058 acc: 0.6636 | lr: 0.0015 | Elapsed: 5.7249 min
Epoch: 155 Train loss: 1.3059 acc: 0.6632 | lr: 0.0015 | Elapsed: 5.7621 min
Epoch: 156 Train loss: 1.2941 acc: 0.664 | lr: 0.0015 | Elapsed: 5.8 min
Epoch: 157 Train loss: 1.2948 acc: 0.6641 | lr: 0.0015 | Elapsed: 5.8373 min
Epoch: 158 Train loss: 1.288 acc: 0.6667 | lr: 0.0015 | Elapsed: 5.8752 min
Epoch: 159 Train loss: 1.2853 acc: 0.6678 | lr: 0.0015 | Elapsed: 5.9128 min
Epoch: 160 Train loss: 1.2868 acc: 0.6665 | lr: 0.0015 | Elapsed: 5.9507 min
Epoch: 161 Train loss: 1.2775 acc: 0.6704 | lr: 0.0015 | Elapsed: 5.9872 min
Epoch: 162 Train loss: 1.2736 acc: 0.6705 | lr: 0.0015 | Elapsed: 6.0208 min
Epoch: 163 Train loss: 1.2802 acc: 0.6705 | lr: 0.0015 | Elapsed: 6.0588 min
Epoch: 164 Train loss: 1.2631 acc: 0.6738 | lr: 0.0015 | Elapsed: 6.096 min
Epoch: 165 Train loss: 1.2635 acc: 0.674 | lr: 0.0015 | Elapsed: 6.1312 min
Epoch: 166 Train loss: 1.2663 acc: 0.6723 | lr: 0.0015 | Elapsed: 6.1661 min
Epoch: 167 Train loss: 1.2562 acc: 0.6759 | lr: 0.0015 | Elapsed: 6.2021 min
Epoch: 168 Train loss: 1.2505 acc: 0.6785 | lr: 0.0014 | Elapsed: 6.2377 min
Epoch: 169 Train loss: 1.2441 acc: 0.6798 | lr: 0.0014 | Elapsed: 6.2744 min
Epoch: 170 Train loss: 1.2441 acc: 0.6816 | lr: 0.0014 | Elapsed: 6.3101 min
Epoch: 171 Train loss: 1.2335 acc: 0.6829 | lr: 0.0014 | Elapsed: 6.3464 min
Epoch: 172 Train loss: 1.2319 acc: 0.6833 | lr: 0.0014 | Elapsed: 6.3793 min
Epoch: 173 Train loss: 1.2282 acc: 0.683 | lr: 0.0014 | Elapsed: 6.4123 min
Epoch: 174 Train loss: 1.2212 acc: 0.6861 | lr: 0.0014 | Elapsed: 6.4453 min
Epoch: 175 Train loss: 1.2213 acc: 0.6863 | lr: 0.0014 | Elapsed: 6.4782 min
Epoch: 176 Train loss: 1.2175 acc: 0.6868 | lr: 0.0014 | Elapsed: 6.5113 min
Epoch: 177 Train loss: 1.2145 acc: 0.6888 | lr: 0.0014 | Elapsed: 6.5444 min
Epoch: 178 Train loss: 1.2101 acc: 0.6867 | lr: 0.0014 | Elapsed: 6.5776 min
Epoch: 179 Train loss: 1.2048 acc: 0.6908 | lr: 0.0014 | Elapsed: 6.6125 min
Epoch: 180 Train loss: 1.1981 acc: 0.6932 | lr: 0.0014 | Elapsed: 6.6462 min
Epoch: 181 Train loss: 1.1899 acc: 0.695 | lr: 0.0014 | Elapsed: 6.6799 min
Epoch: 182 Train loss: 1.1818 acc: 0.6976 | lr: 0.0014 | Elapsed: 6.7136 min
Epoch: 183 Train loss: 1.1897 acc: 0.6932 | lr: 0.0014 | Elapsed: 6.7473 min
Epoch: 184 Train loss: 1.186 acc: 0.6963 | lr: 0.0014 | Elapsed: 6.781 min
Epoch: 185 Train loss: 1.181 acc: 0.6979 | lr: 0.0014 | Elapsed: 6.8146 min
Epoch: 186 Train loss: 1.1814 acc: 0.6976 | lr: 0.0014 | Elapsed: 6.8493 min
Epoch: 187 Train loss: 1.1752 acc: 0.698 | lr: 0.0014 | Elapsed: 6.8829 min
Epoch: 188 Train loss: 1.1714 acc: 0.6988 | lr: 0.0014 | Elapsed: 6.9169 min
Epoch: 189 Train loss: 1.1683 acc: 0.7006 | lr: 0.0014 | Elapsed: 6.9506 min
Epoch: 190 Train loss: 1.1637 acc: 0.7018 | lr: 0.0014 | Elapsed: 6.9845 min
Epoch: 191 Train loss: 1.1598 acc: 0.7041 | lr: 0.0014 | Elapsed: 7.0182 min
Epoch: 192 Train loss: 1.1579 acc: 0.7031 | lr: 0.0014 | Elapsed: 7.052 min
Epoch: 193 Train loss: 1.152 acc: 0.7071 | lr: 0.0014 | Elapsed: 7.0879 min
Epoch: 194 Train loss: 1.1484 acc: 0.7062 | lr: 0.0013 | Elapsed: 7.1232 min
Epoch: 195 Train loss: 1.1457 acc: 0.7056 | lr: 0.0013 | Elapsed: 7.1595 min
Epoch: 196 Train loss: 1.1483 acc: 0.7072 | lr: 0.0013 | Elapsed: 7.1952 min
Epoch: 197 Train loss: 1.1378 acc: 0.7101 | lr: 0.0013 | Elapsed: 7.2341 min
Epoch: 198 Train loss: 1.1447 acc: 0.7079 | lr: 0.0013 | Elapsed: 7.273 min
Epoch: 199 Train loss: 1.1369 acc: 0.7106 | lr: 0.0013 | Elapsed: 7.3123 min
Epoch: 200 Train loss: 1.1307 acc: 0.7127 | lr: 0.0013 | Elapsed: 7.3507 min
Epoch: 201 Train loss: 1.1286 acc: 0.712 | lr: 0.0013 | Elapsed: 7.3899 min
Epoch: 202 Train loss: 1.1254 acc: 0.7126 | lr: 0.0013 | Elapsed: 7.4283 min
Epoch: 203 Train loss: 1.1246 acc: 0.7124 | lr: 0.0013 | Elapsed: 7.4665 min
Epoch: 204 Train loss: 1.1189 acc: 0.7152 | lr: 0.0013 | Elapsed: 7.5049 min
Epoch: 205 Train loss: 1.1245 acc: 0.7132 | lr: 0.0013 | Elapsed: 7.5434 min
Epoch: 206 Train loss: 1.1191 acc: 0.7151 | lr: 0.0013 | Elapsed: 7.5817 min
Epoch: 207 Train loss: 1.1129 acc: 0.7161 | lr: 0.0013 | Elapsed: 7.6203 min
Epoch: 208 Train loss: 1.1061 acc: 0.7189 | lr: 0.0013 | Elapsed: 7.659 min
Epoch: 209 Train loss: 1.1028 acc: 0.7197 | Evaluation on Test Set
Evaluation Phase over 193 sentences-----------------------
0) -------------------------- Input: ['chunki', 'dunyo', 'muammo@@', 'lari', 'inson@@', 'larning', 'o@@', 'ila@@', 'viy', 'mer@@', 'o@@', 'si', "bo'lishi", 'kerak', 'emas', '.']
Pred: ['because', 'it', "'s", 'not', 'a', 'fundamentally', 'equities', '.']
Gt: [['because', 'the', 'world', "'s", 'problems', 'shouldn', "'t", 'be', 'the', 'human', 'family', "'s", 'heirloom', '.']]
1) -------------------------- Input: ['(', '<UNK>', ')']
Pred: ['(', 'laughter', ')']
Gt: [['(', 'applause', ')']]
2) -------------------------- Input: ['li@@', 'der@@', 'lik', ',', 'boshqar@@', 'ish', 'ker@@', 'a@@', 'gi@@', 'dan', 'or@@', 'ti@@', 'q@@', 'cha', 'maq@@', 'ta@@', 'ladi', ',', 'ana', 'u', "ko'@@", 'y@@', 'lak@@', 'siz', 'kishi', 'birinchi', 'bosh@@', 'ladi', 'va', 'hamma', 'sh@@', 'ara@@', 'f', 'unga', 'te@@', 'ga@@', 'di', ',', 'lekin', 'aslida', 'uni', 'li@@', 'der@@', 'ga', 'aylan@@', 'tir@@', 'gan', 'odam', 'ana', 'shu', 'birinchi', "qo'sh@@", 'ilgan', 'odam', "bo'ladi", '.']
Pred: ['how', 'do', 'they', 'want', 'to', 'control', '?', '(', 'laughter', ')', 'but', 'in', 'the', 'age', 'of', 'time', ',', 'anage', 'of', 'his', 'brain', ',', '1,', '000', 'years', 'ago', ',', 'and', 'in', 'the', 'house', 'of', '<UNK>', '.']
Gt: [['yes', ',', 'it', 'was', 'the', 'shirtless', 'guy', 'who', 'was', 'first', ',', 'and', 'he', "'ll", 'get', 'all', 'the', 'credit', ',', 'but', 'it', 'was', 'really', 'the', 'first', 'follower', 'that', 'transformed', 'the', 'lone', 'nut', 'into', 'a', 'leader', '.']]
3) -------------------------- Input: ['u', 'yana', 'bir', 'yil', 'dar@@', 's@@', 'lar', 'olib', 'mash@@', 'q', 'qiladi', ',', 'va', 'endi', 'u', "to'@@", 'q@@', 'qi@@', 'z', 'yosh@@', 'da', '.']
Pred: ['now', ',', 'he', 'practices', 'for', 'a', 'year', 'and', 'takes', 'lessons', ',', 'he', "'s", '10', '.']
Gt: [['he', 'practices', 'for', 'another', 'year', 'and', 'takes', 'lessons', '--', 'he', "'s", 'nine', '.']]
SacreBLEU: BLEU = 16.27 37.7/17.7/14.4/13.2 (BP = 0.863 ratio = 0.871 hyp_len = 3328 ref_len = 3819)
Test Bleu: 16.27 | lr: 0.0013 | Elapsed: 7.8003 min
Epoch: 210 Train loss: 1.1002 acc: 0.7198 | lr: 0.0013 | Elapsed: 7.8385 min
Epoch: 211 Train loss: 1.1073 acc: 0.7164 | lr: 0.0013 | Elapsed: 7.877 min
Epoch: 212 Train loss: 1.1027 acc: 0.7203 | lr: 0.0013 | Elapsed: 7.9154 min
Epoch: 213 Train loss: 1.0979 acc: 0.72 | lr: 0.0013 | Elapsed: 7.9538 min
Epoch: 214 Train loss: 1.0968 acc: 0.7228 | lr: 0.0013 | Elapsed: 7.9924 min
Epoch: 215 Train loss: 1.093 acc: 0.7229 | lr: 0.0013 | Elapsed: 8.031 min
Epoch: 216 Train loss: 1.087 acc: 0.7247 | lr: 0.0013 | Elapsed: 8.069 min
Epoch: 217 Train loss: 1.0855 acc: 0.7251 | lr: 0.0013 | Elapsed: 8.1074 min
Epoch: 218 Train loss: 1.0787 acc: 0.728 | lr: 0.0013 | Elapsed: 8.1447 min
Epoch: 219 Train loss: 1.0807 acc: 0.7279 | Evaluation on Test Set
Evaluation Phase over 193 sentences-----------------------
/home/jchu/Progetti/lowres_repo_seth/test_withBPE.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  batch_src = torch.tensor(batch_src).to(model.rank)
0) -------------------------- Input: ['chunki', 'dunyo', 'muammo@@', 'lari', 'inson@@', 'larning', 'o@@', 'ila@@', 'viy', 'mer@@', 'o@@', 'si', "bo'lishi", 'kerak', 'emas', '.']
Pred: ['it', "'s", 'not', 'entirely', 'through', '.']
Gt: [['because', 'the', 'world', "'s", 'problems', 'shouldn', "'t", 'be', 'the', 'human', 'family', "'s", 'heirloom', '.']]
1) -------------------------- Input: ['(', '<UNK>', ')']
Pred: ['(', '<UNK>', ')']
Gt: [['(', 'applause', ')']]
2) -------------------------- Input: ['li@@', 'der@@', 'lik', ',', 'boshqar@@', 'ish', 'ker@@', 'a@@', 'gi@@', 'dan', 'or@@', 'ti@@', 'q@@', 'cha', 'maq@@', 'ta@@', 'ladi', ',', 'ana', 'u', "ko'@@", 'y@@', 'lak@@', 'siz', 'kishi', 'birinchi', 'bosh@@', 'ladi', 'va', 'hamma', 'sh@@', 'ara@@', 'f', 'unga', 'te@@', 'ga@@', 'di', ',', 'lekin', 'aslida', 'uni', 'li@@', 'der@@', 'ga', 'aylan@@', 'tir@@', 'gan', 'odam', 'ana', 'shu', 'birinchi', "qo'sh@@", 'ilgan', 'odam', "bo'ladi", '.']
Pred: ['how', 'do', 'you', 'find', 'out', 'the', 'age', 'of', '25', ',', 'just', 'plain', 'the', 'breakes', ',', 'or', 'when', 'he', 'was', 'eight', ',', 'or', 'just', 'about', 'refrigeranage', 'of', 'interview', '<UNK>', '.']
Gt: [['yes', ',', 'it', 'was', 'the', 'shirtless', 'guy', 'who', 'was', 'first', ',', 'and', 'he', "'ll", 'get', 'all', 'the', 'credit', ',', 'but', 'it', 'was', 'really', 'the', 'first', 'follower', 'that', 'transformed', 'the', 'lone', 'nut', 'into', 'a', 'leader', '.']]
3) -------------------------- Input: ['u', 'yana', 'bir', 'yil', 'dar@@', 's@@', 'lar', 'olib', 'mash@@', 'q', 'qiladi', ',', 'va', 'endi', 'u', "to'@@", 'q@@', 'qi@@', 'z', 'yosh@@', 'da', '.']
Pred: ['now', ',', 'he', 'practices', 'for', 'a', 'year', 'and', 'takes', 'lessons', '.']
Gt: [['he', 'practices', 'for', 'another', 'year', 'and', 'takes', 'lessons', '--', 'he', "'s", 'nine', '.']]
SacreBLEU: BLEU = 15.86 38.6/18.4/15.2/14.1 (BP = 0.803 ratio = 0.820 hyp_len = 3131 ref_len = 3819)
Test Bleu: 15.86 | lr: 0.0013 | Elapsed: 8.2961 min
Epoch: 220 Train loss: 1.0802 acc: 0.7275 | lr: 0.0013 | Elapsed: 8.3354 min
Epoch: 221 Train loss: 1.0743 acc: 0.7282 | lr: 0.0013 | Elapsed: 8.3734 min
Epoch: 222 Train loss: 1.0703 acc: 0.7314 | lr: 0.0013 | Elapsed: 8.4115 min
Epoch: 223 Train loss: 1.0705 acc: 0.7299 | lr: 0.0013 | Elapsed: 8.449 min
Epoch: 224 Train loss: 1.0637 acc: 0.733 | lr: 0.0013 | Elapsed: 8.4873 min
Epoch: 225 Train loss: 1.0617 acc: 0.7327 | lr: 0.0013 | Elapsed: 8.5255 min
Epoch: 226 Train loss: 1.0652 acc: 0.7305 | lr: 0.0013 | Elapsed: 8.5634 min
Epoch: 227 Train loss: 1.065 acc: 0.7324 | lr: 0.0012 | Elapsed: 8.6011 min
Epoch: 228 Train loss: 1.0535 acc: 0.7361 | lr: 0.0012 | Elapsed: 8.639 min
Epoch: 229 Train loss: 1.0546 acc: 0.7354 | Evaluation on Test Set
Evaluation Phase over 193 sentences-----------------------
0) -------------------------- Input: ['chunki', 'dunyo', 'muammo@@', 'lari', 'inson@@', 'larning', 'o@@', 'ila@@', 'viy', 'mer@@', 'o@@', 'si', "bo'lishi", 'kerak', 'emas', '.']
Pred: ['because', 'it', "'s", 'not', 'a', 'fundamentally', 'equities', '.']
Gt: [['because', 'the', 'world', "'s", 'problems', 'shouldn', "'t", 'be', 'the', 'human', 'family', "'s", 'heirloom', '.']]
1) -------------------------- Input: ['(', '<UNK>', ')']
Pred: ['(', 'laughter', ')']
Gt: [['(', 'applause', ')']]
2) -------------------------- Input: ['li@@', 'der@@', 'lik', ',', 'boshqar@@', 'ish', 'ker@@', 'a@@', 'gi@@', 'dan', 'or@@', 'ti@@', 'q@@', 'cha', 'maq@@', 'ta@@', 'ladi', ',', 'ana', 'u', "ko'@@", 'y@@', 'lak@@', 'siz', 'kishi', 'birinchi', 'bosh@@', 'ladi', 'va', 'hamma', 'sh@@', 'ara@@', 'f', 'unga', 'te@@', 'ga@@', 'di', ',', 'lekin', 'aslida', 'uni', 'li@@', 'der@@', 'ga', 'aylan@@', 'tir@@', 'gan', 'odam', 'ana', 'shu', 'birinchi', "qo'sh@@", 'ilgan', 'odam', "bo'ladi", '.']
Pred: ['(', 'laughter', ')']
Gt: [['yes', ',', 'it', 'was', 'the', 'shirtless', 'guy', 'who', 'was', 'first', ',', 'and', 'he', "'ll", 'get', 'all', 'the', 'credit', ',', 'but', 'it', 'was', 'really', 'the', 'first', 'follower', 'that', 'transformed', 'the', 'lone', 'nut', 'into', 'a', 'leader', '.']]
3) -------------------------- Input: ['u', 'yana', 'bir', 'yil', 'dar@@', 's@@', 'lar', 'olib', 'mash@@', 'q', 'qiladi', ',', 'va', 'endi', 'u', "to'@@", 'q@@', 'qi@@', 'z', 'yosh@@', 'da', '.']
Pred: ['now', ',', 'he', 'practices', 'for', 'a', 'year', 'and', 'takes', 'lessons', '--', 'he', "'s", '10', '.']
Gt: [['he', 'practices', 'for', 'another', 'year', 'and', 'takes', 'lessons', '--', 'he', "'s", 'nine', '.']]
SacreBLEU: BLEU = 14.76 37.2/17.8/14.1/12.7 (BP = 0.794 ratio = 0.813 hyp_len = 3104 ref_len = 3819)
Test Bleu: 14.76 | lr: 0.0012 | Elapsed: 8.8167 min
Epoch: 230 Train loss: 1.0539 acc: 0.7359 | lr: 0.0012 | Elapsed: 8.8557 min
Epoch: 231 Train loss: 1.0476 acc: 0.7376 | lr: 0.0012 | Elapsed: 8.8951 min
Epoch: 232 Train loss: 1.0446 acc: 0.7378 | lr: 0.0012 | Elapsed: 8.9343 min
Epoch: 233 Train loss: 1.0463 acc: 0.7362 | lr: 0.0012 | Elapsed: 8.9731 min
Epoch: 234 Train loss: 1.0408 acc: 0.7389 | lr: 0.0012 | Elapsed: 9.0122 min
Epoch: 235 Train loss: 1.0417 acc: 0.7397 | lr: 0.0012 | Elapsed: 9.0505 min
Epoch: 236 Train loss: 1.0371 acc: 0.7396 | lr: 0.0012 | Elapsed: 9.0887 min
Epoch: 237 Train loss: 1.0339 acc: 0.7412 | lr: 0.0012 | Elapsed: 9.1267 min
Epoch: 238 Train loss: 1.0373 acc: 0.7416 | lr: 0.0012 | Elapsed: 9.1649 min
Epoch: 239 Train loss: 1.0332 acc: 0.7416 | Evaluation on Test Set
Evaluation Phase over 193 sentences-----------------------
0) -------------------------- Input: ['chunki', 'dunyo', 'muammo@@', 'lari', 'inson@@', 'larning', 'o@@', 'ila@@', 'viy', 'mer@@', 'o@@', 'si', "bo'lishi", 'kerak', 'emas', '.']
Pred: ['because', 'it', "'s", 'not', 'a', 'fundamentally', 'equities', '.']
Gt: [['because', 'the', 'world', "'s", 'problems', 'shouldn', "'t", 'be', 'the', 'human', 'family', "'s", 'heirloom', '.']]
1) -------------------------- Input: ['(', '<UNK>', ')']
Pred: ['(', '<UNK>', ')']
Gt: [['(', 'applause', ')']]
2) -------------------------- Input: ['li@@', 'der@@', 'lik', ',', 'boshqar@@', 'ish', 'ker@@', 'a@@', 'gi@@', 'dan', 'or@@', 'ti@@', 'q@@', 'cha', 'maq@@', 'ta@@', 'ladi', ',', 'ana', 'u', "ko'@@", 'y@@', 'lak@@', 'siz', 'kishi', 'birinchi', 'bosh@@', 'ladi', 'va', 'hamma', 'sh@@', 'ara@@', 'f', 'unga', 'te@@', 'ga@@', 'di', ',', 'lekin', 'aslida', 'uni', 'li@@', 'der@@', 'ga', 'aylan@@', 'tir@@', 'gan', 'odam', 'ana', 'shu', 'birinchi', "qo'sh@@", 'ilgan', 'odam', "bo'ladi", '.']
Pred: ['(', 'laughter', ')']
Gt: [['yes', ',', 'it', 'was', 'the', 'shirtless', 'guy', 'who', 'was', 'first', ',', 'and', 'he', "'ll", 'get', 'all', 'the', 'credit', ',', 'but', 'it', 'was', 'really', 'the', 'first', 'follower', 'that', 'transformed', 'the', 'lone', 'nut', 'into', 'a', 'leader', '.']]
3) -------------------------- Input: ['u', 'yana', 'bir', 'yil', 'dar@@', 's@@', 'lar', 'olib', 'mash@@', 'q', 'qiladi', ',', 'va', 'endi', 'u', "to'@@", 'q@@', 'qi@@', 'z', 'yosh@@', 'da', '.']
Pred: ['now', ',', 'he', 'practices', 'for', 'a', 'year', 'and', 'takes', 'lessons', '--', 'back', '.']
Gt: [['he', 'practices', 'for', 'another', 'year', 'and', 'takes', 'lessons', '--', 'he', "'s", 'nine', '.']]
SacreBLEU: BLEU = 16.67 38.3/18.3/15.0/13.6 (BP = 0.857 ratio = 0.866 hyp_len = 3308 ref_len = 3819)
Test Bleu: 16.67 | lr: 0.0012 | Elapsed: 9.3303 min
Epoch: 240 Train loss: 1.0276 acc: 0.7439 | lr: 0.0012 | Elapsed: 9.3695 min
Epoch: 241 Train loss: 1.0235 acc: 0.7436 | lr: 0.0012 | Elapsed: 9.4076 min
Epoch: 242 Train loss: 1.0276 acc: 0.7435 | lr: 0.0012 | Elapsed: 9.4455 min
Epoch: 243 Train loss: 1.0196 acc: 0.7456 | lr: 0.0012 | Elapsed: 9.484 min
Epoch: 244 Train loss: 1.0163 acc: 0.7466 | lr: 0.0012 | Elapsed: 9.5221 min
Epoch: 245 Train loss: 1.0206 acc: 0.7462 | lr: 0.0012 | Elapsed: 9.5601 min
Epoch: 246 Train loss: 1.0171 acc: 0.7448 | lr: 0.0012 | Elapsed: 9.598 min
Epoch: 247 Train loss: 1.0197 acc: 0.7457 | lr: 0.0012 | Elapsed: 9.637 min
Epoch: 248 Train loss: 1.0097 acc: 0.7479 | lr: 0.0012 | Elapsed: 9.676 min
Epoch: 249 Train loss: 1.0147 acc: 0.7482 | Evaluation on Test Set
Evaluation Phase over 193 sentences-----------------------
0) -------------------------- Input: ['chunki', 'dunyo', 'muammo@@', 'lari', 'inson@@', 'larning', 'o@@', 'ila@@', 'viy', 'mer@@', 'o@@', 'si', "bo'lishi", 'kerak', 'emas', '.']
Pred: ['because', 'it', "'s", 'not', 'a', 'fundamentally', 'equities', '.']
Gt: [['because', 'the', 'world', "'s", 'problems', 'shouldn', "'t", 'be', 'the', 'human', 'family', "'s", 'heirloom', '.']]
1) -------------------------- Input: ['(', '<UNK>', ')']
Pred: ['(', '<UNK>', ')']
Gt: [['(', 'applause', ')']]
2) -------------------------- Input: ['li@@', 'der@@', 'lik', ',', 'boshqar@@', 'ish', 'ker@@', 'a@@', 'gi@@', 'dan', 'or@@', 'ti@@', 'q@@', 'cha', 'maq@@', 'ta@@', 'ladi', ',', 'ana', 'u', "ko'@@", 'y@@', 'lak@@', 'siz', 'kishi', 'birinchi', 'bosh@@', 'ladi', 'va', 'hamma', 'sh@@', 'ara@@', 'f', 'unga', 'te@@', 'ga@@', 'di', ',', 'lekin', 'aslida', 'uni', 'li@@', 'der@@', 'ga', 'aylan@@', 'tir@@', 'gan', 'odam', 'ana', 'shu', 'birinchi', "qo'sh@@", 'ilgan', 'odam', "bo'ladi", '.']
Pred: ['how', 'many', 'imperfect', 'becomes', 'to', 'control', 'their', 'pain', ',', 'so', 'they', 'would', 'stop', 'receive', 'the', 'dancer', 'as', 'much', 'as', 'anage', ',', 'and', 'says', ',', 'in', 'the', 'dancer', '.', '(', 'laughter', ')']
Gt: [['yes', ',', 'it', 'was', 'the', 'shirtless', 'guy', 'who', 'was', 'first', ',', 'and', 'he', "'ll", 'get', 'all', 'the', 'credit', ',', 'but', 'it', 'was', 'really', 'the', 'first', 'follower', 'that', 'transformed', 'the', 'lone', 'nut', 'into', 'a', 'leader', '.']]
3) -------------------------- Input: ['u', 'yana', 'bir', 'yil', 'dar@@', 's@@', 'lar', 'olib', 'mash@@', 'q', 'qiladi', ',', 'va', 'endi', 'u', "to'@@", 'q@@', 'qi@@', 'z', 'yosh@@', 'da', '.']
Pred: ['now', ',', 'he', 'practices', 'for', 'a', 'year', 'and', 'takes', 'lessons', '--', 'he', "'s", '10', '.']
Gt: [['he', 'practices', 'for', 'another', 'year', 'and', 'takes', 'lessons', '--', 'he', "'s", 'nine', '.']]
SacreBLEU: BLEU = 16.83 37.6/18.3/14.9/13.6 (BP = 0.870 ratio = 0.878 hyp_len = 3353 ref_len = 3819)
Test Bleu: 16.83 | lr: 0.0012 | Elapsed: 9.8182 min
Epoch: 250 Train loss: 1.0066 acc: 0.7494 | lr: 0.0012 | Elapsed: 9.8572 min
Epoch: 251 Train loss: 1.0034 acc: 0.7505 | lr: 0.0012 | Elapsed: 9.8953 min
Epoch: 252 Train loss: 1.009 acc: 0.7488 | lr: 0.0012 | Elapsed: 9.9331 min
Epoch: 253 Train loss: 1.0013 acc: 0.7516 | lr: 0.0012 | Elapsed: 9.971 min
Epoch: 254 Train loss: 1.0077 acc: 0.749 | lr: 0.0012 | Elapsed: 10.0091 min
Epoch: 255 Train loss: 1.0001 acc: 0.7515 | lr: 0.0012 | Elapsed: 10.0472 min
Epoch: 256 Train loss: 0.9969 acc: 0.7533 | lr: 0.0012 | Elapsed: 10.0857 min
Epoch: 257 Train loss: 1.0017 acc: 0.7508 | lr: 0.0012 | Elapsed: 10.1237 min
Epoch: 258 Train loss: 0.9925 acc: 0.7548 | lr: 0.0012 | Elapsed: 10.1621 min
Epoch: 259 Train loss: 0.9967 acc: 0.7517 | Evaluation on Test Set
Evaluation Phase over 193 sentences-----------------------
0) -------------------------- Input: ['chunki', 'dunyo', 'muammo@@', 'lari', 'inson@@', 'larning', 'o@@', 'ila@@', 'viy', 'mer@@', 'o@@', 'si', "bo'lishi", 'kerak', 'emas', '.']
Pred: ['it', "'s", 'not', 'a', 'single-', 'function', '.']
Gt: [['because', 'the', 'world', "'s", 'problems', 'shouldn', "'t", 'be', 'the', 'human', 'family', "'s", 'heirloom', '.']]
1) -------------------------- Input: ['(', '<UNK>', ')']
Pred: ['(', '<UNK>', ')']
Gt: [['(', 'applause', ')']]
2) -------------------------- Input: ['li@@', 'der@@', 'lik', ',', 'boshqar@@', 'ish', 'ker@@', 'a@@', 'gi@@', 'dan', 'or@@', 'ti@@', 'q@@', 'cha', 'maq@@', 'ta@@', 'ladi', ',', 'ana', 'u', "ko'@@", 'y@@', 'lak@@', 'siz', 'kishi', 'birinchi', 'bosh@@', 'ladi', 'va', 'hamma', 'sh@@', 'ara@@', 'f', 'unga', 'te@@', 'ga@@', 'di', ',', 'lekin', 'aslida', 'uni', 'li@@', 'der@@', 'ga', 'aylan@@', 'tir@@', 'gan', 'odam', 'ana', 'shu', 'birinchi', "qo'sh@@", 'ilgan', 'odam', "bo'ladi", '.']
Pred: ['how', 'do', 'you', 'find', 'out', 'about', 'the', 'control', '?', 'and', 'she', 'was', 'happening', 'at', 'the', 'dancer', '?', 'or', 'when', 'he', 'was', 'in', 'the', 'age', ',', 'and', 'he', 'was', 'just', 'keeping', 'up', 'with', 'her', 'baby', '<UNK>', '.']
Gt: [['yes', ',', 'it', 'was', 'the', 'shirtless', 'guy', 'who', 'was', 'first', ',', 'and', 'he', "'ll", 'get', 'all', 'the', 'credit', ',', 'but', 'it', 'was', 'really', 'the', 'first', 'follower', 'that', 'transformed', 'the', 'lone', 'nut', 'into', 'a', 'leader', '.']]
3) -------------------------- Input: ['u', 'yana', 'bir', 'yil', 'dar@@', 's@@', 'lar', 'olib', 'mash@@', 'q', 'qiladi', ',', 'va', 'endi', 'u', "to'@@", 'q@@', 'qi@@', 'z', 'yosh@@', 'da', '.']
Pred: ['now', ',', 'he', 'practices', 'for', 'a', 'year', 'and', 'takes', 'lessons', '.']
Gt: [['he', 'practices', 'for', 'another', 'year', 'and', 'takes', 'lessons', '--', 'he', "'s", 'nine', '.']]
SacreBLEU: BLEU = 17.29 38.9/19.0/15.8/14.7 (BP = 0.850 ratio = 0.860 hyp_len = 3284 ref_len = 3819)
Test Bleu: 17.29 | lr: 0.0012 | Elapsed: 10.3883 min
Epoch: 260 Train loss: 0.988 acc: 0.754 | lr: 0.0012 | Elapsed: 10.4176 min
Epoch: 261 Train loss: 0.9837 acc: 0.7567 | lr: 0.0012 | Elapsed: 10.4562 min
Epoch: 262 Train loss: 0.9843 acc: 0.7574 | lr: 0.0012 | Elapsed: 10.4942 min
Epoch: 263 Train loss: 0.9829 acc: 0.7564 | lr: 0.0012 | Elapsed: 10.532 min
Epoch: 264 Train loss: 0.9858 acc: 0.7578 | lr: 0.0012 | Elapsed: 10.5704 min
Epoch: 265 Train loss: 0.9798 acc: 0.7588 | lr: 0.0012 | Elapsed: 10.6088 min
Epoch: 266 Train loss: 0.9802 acc: 0.758 | lr: 0.0012 | Elapsed: 10.6472 min
Epoch: 267 Train loss: 0.9723 acc: 0.7614 | lr: 0.0012 | Elapsed: 10.6859 min
Epoch: 268 Train loss: 0.9759 acc: 0.7604 | lr: 0.0011 | Elapsed: 10.7244 min
Epoch: 269 Train loss: 0.9717 acc: 0.7606 | Evaluation on Test Set
Evaluation Phase over 193 sentences-----------------------
0) -------------------------- Input: ['chunki', 'dunyo', 'muammo@@', 'lari', 'inson@@', 'larning', 'o@@', 'ila@@', 'viy', 'mer@@', 'o@@', 'si', "bo'lishi", 'kerak', 'emas', '.']
Pred: ['because', 'it', "'s", 'not', 'a', 'fundamentally', 'equities', '.']
Gt: [['because', 'the', 'world', "'s", 'problems', 'shouldn', "'t", 'be', 'the', 'human', 'family', "'s", 'heirloom', '.']]
1) -------------------------- Input: ['(', '<UNK>', ')']
Pred: ['(', '<UNK>', ')']
Gt: [['(', 'applause', ')']]
2) -------------------------- Input: ['li@@', 'der@@', 'lik', ',', 'boshqar@@', 'ish', 'ker@@', 'a@@', 'gi@@', 'dan', 'or@@', 'ti@@', 'q@@', 'cha', 'maq@@', 'ta@@', 'ladi', ',', 'ana', 'u', "ko'@@", 'y@@', 'lak@@', 'siz', 'kishi', 'birinchi', 'bosh@@', 'ladi', 'va', 'hamma', 'sh@@', 'ara@@', 'f', 'unga', 'te@@', 'ga@@', 'di', ',', 'lekin', 'aslida', 'uni', 'li@@', 'der@@', 'ga', 'aylan@@', 'tir@@', 'gan', 'odam', 'ana', 'shu', 'birinchi', "qo'sh@@", 'ilgan', 'odam', "bo'ladi", '.']
Pred: ['(', 'laughter', ')']
Gt: [['yes', ',', 'it', 'was', 'the', 'shirtless', 'guy', 'who', 'was', 'first', ',', 'and', 'he', "'ll", 'get', 'all', 'the', 'credit', ',', 'but', 'it', 'was', 'really', 'the', 'first', 'follower', 'that', 'transformed', 'the', 'lone', 'nut', 'into', 'a', 'leader', '.']]
3) -------------------------- Input: ['u', 'yana', 'bir', 'yil', 'dar@@', 's@@', 'lar', 'olib', 'mash@@', 'q', 'qiladi', ',', 'va', 'endi', 'u', "to'@@", 'q@@', 'qi@@', 'z', 'yosh@@', 'da', '.']
Pred: ['now', ',', 'he', 'practices', 'for', 'a', 'year', 'and', 'takes', 'lessons', '--', 'he', "'s", '10', '.']
Gt: [['he', 'practices', 'for', 'another', 'year', 'and', 'takes', 'lessons', '--', 'he', "'s", 'nine', '.']]
SacreBLEU: BLEU = 17.70 38.8/19.9/16.4/15.0 (BP = 0.849 ratio = 0.859 hyp_len = 3281 ref_len = 3819)
Test Bleu: 17.70 | lr: 0.0011 | Elapsed: 10.8801 min
Epoch: 270 Train loss: 0.9754 acc: 0.7599 | lr: 0.0011 | Elapsed: 10.9183 min
Epoch: 271 Train loss: 0.966 acc: 0.7612 | lr: 0.0011 | Elapsed: 10.957 min
Epoch: 272 Train loss: 0.97 acc: 0.762 | lr: 0.0011 | Elapsed: 10.9956 min
Epoch: 273 Train loss: 0.9696 acc: 0.7611 | lr: 0.0011 | Elapsed: 11.0341 min
Epoch: 274 Train loss: 0.9632 acc: 0.7627 | lr: 0.0011 | Elapsed: 11.0727 min
Epoch: 275 Train loss: 0.963 acc: 0.7629 | lr: 0.0011 | Elapsed: 11.1112 min
Epoch: 276 Train loss: 0.9627 acc: 0.7623 | lr: 0.0011 | Elapsed: 11.1496 min
Epoch: 277 Train loss: 0.9615 acc: 0.7645 | lr: 0.0011 | Elapsed: 11.1883 min
Epoch: 278 Train loss: 0.9602 acc: 0.765 | lr: 0.0011 | Elapsed: 11.2268 min
Epoch: 279 Train loss: 0.9553 acc: 0.7656 | Evaluation on Test Set
Evaluation Phase over 193 sentences-----------------------
0) -------------------------- Input: ['chunki', 'dunyo', 'muammo@@', 'lari', 'inson@@', 'larning', 'o@@', 'ila@@', 'viy', 'mer@@', 'o@@', 'si', "bo'lishi", 'kerak', 'emas', '.']
Pred: ['because', 'it', "'s", 'not', 'a', 'single-', 'function', '.']
Gt: [['because', 'the', 'world', "'s", 'problems', 'shouldn', "'t", 'be', 'the', 'human', 'family', "'s", 'heirloom', '.']]
1) -------------------------- Input: ['(', '<UNK>', ')']
Pred: ['(', 'laughter', ')']
Gt: [['(', 'applause', ')']]
2) -------------------------- Input: ['li@@', 'der@@', 'lik', ',', 'boshqar@@', 'ish', 'ker@@', 'a@@', 'gi@@', 'dan', 'or@@', 'ti@@', 'q@@', 'cha', 'maq@@', 'ta@@', 'ladi', ',', 'ana', 'u', "ko'@@", 'y@@', 'lak@@', 'siz', 'kishi', 'birinchi', 'bosh@@', 'ladi', 'va', 'hamma', 'sh@@', 'ara@@', 'f', 'unga', 'te@@', 'ga@@', 'di', ',', 'lekin', 'aslida', 'uni', 'li@@', 'der@@', 'ga', 'aylan@@', 'tir@@', 'gan', 'odam', 'ana', 'shu', 'birinchi', "qo'sh@@", 'ilgan', 'odam', "bo'ladi", '.']
Pred: ['how', 'many', 'impert', 'the', 'age', 'of', 'myself', ',', 'as', 'an', 'when', 'she', 'was', ',', 'and', 'or', 'when', 'she', 'was', 'distinherence', ',', 'as', 'a', 'movement', '.', '(', 'laughter', ')']
Gt: [['yes', ',', 'it', 'was', 'the', 'shirtless', 'guy', 'who', 'was', 'first', ',', 'and', 'he', "'ll", 'get', 'all', 'the', 'credit', ',', 'but', 'it', 'was', 'really', 'the', 'first', 'follower', 'that', 'transformed', 'the', 'lone', 'nut', 'into', 'a', 'leader', '.']]
3) -------------------------- Input: ['u', 'yana', 'bir', 'yil', 'dar@@', 's@@', 'lar', 'olib', 'mash@@', 'q', 'qiladi', ',', 'va', 'endi', 'u', "to'@@", 'q@@', 'qi@@', 'z', 'yosh@@', 'da', '.']
Pred: ['now', ',', 'he', 'practices', 'for', 'a', 'year', 'and', 'takes', 'lessons', '--', 'he', "'s", '10', '.']
Gt: [['he', 'practices', 'for', 'another', 'year', 'and', 'takes', 'lessons', '--', 'he', "'s", 'nine', '.']]
SacreBLEU: BLEU = 17.83 38.9/19.7/16.5/15.2 (BP = 0.851 ratio = 0.861 hyp_len = 3290 ref_len = 3819)
Test Bleu: 17.83 | lr: 0.0011 | Elapsed: 11.3943 min
Epoch: 280 Train loss: 0.9527 acc: 0.7665 | lr: 0.0011 | Elapsed: 11.4292 min
Epoch: 281 Train loss: 0.9616 acc: 0.7644 | lr: 0.0011 | Elapsed: 11.4625 min
Epoch: 282 Train loss: 0.9509 acc: 0.765 | lr: 0.0011 | Elapsed: 11.4958 min
Epoch: 283 Train loss: 0.952 acc: 0.768 | lr: 0.0011 | Elapsed: 11.5292 min
Epoch: 284 Train loss: 0.9495 acc: 0.7686 | lr: 0.0011 | Elapsed: 11.5634 min
Epoch: 285 Train loss: 0.9455 acc: 0.7702 | lr: 0.0011 | Elapsed: 11.5969 min
Epoch: 286 Train loss: 0.9443 acc: 0.7688 | lr: 0.0011 | Elapsed: 11.6304 min
Epoch: 287 Train loss: 0.94 acc: 0.7717 | lr: 0.0011 | Elapsed: 11.6638 min
Epoch: 288 Train loss: 0.9424 acc: 0.7689 | lr: 0.0011 | Elapsed: 11.6975 min
Epoch: 289 Train loss: 0.9484 acc: 0.7688 | Evaluation on Test Set
Evaluation Phase over 193 sentences-----------------------
0) -------------------------- Input: ['chunki', 'dunyo', 'muammo@@', 'lari', 'inson@@', 'larning', 'o@@', 'ila@@', 'viy', 'mer@@', 'o@@', 'si', "bo'lishi", 'kerak', 'emas', '.']
Pred: ['because', 'it', "'s", 'not', 'a', 'fundamentally', 'equities', '.']
Gt: [['because', 'the', 'world', "'s", 'problems', 'shouldn', "'t", 'be', 'the', 'human', 'family', "'s", 'heirloom', '.']]
1) -------------------------- Input: ['(', '<UNK>', ')']
Pred: ['(', '<UNK>', ')']
Gt: [['(', 'applause', ')']]
2) -------------------------- Input: ['li@@', 'der@@', 'lik', ',', 'boshqar@@', 'ish', 'ker@@', 'a@@', 'gi@@', 'dan', 'or@@', 'ti@@', 'q@@', 'cha', 'maq@@', 'ta@@', 'ladi', ',', 'ana', 'u', "ko'@@", 'y@@', 'lak@@', 'siz', 'kishi', 'birinchi', 'bosh@@', 'ladi', 'va', 'hamma', 'sh@@', 'ara@@', 'f', 'unga', 'te@@', 'ga@@', 'di', ',', 'lekin', 'aslida', 'uni', 'li@@', 'der@@', 'ga', 'aylan@@', 'tir@@', 'gan', 'odam', 'ana', 'shu', 'birinchi', "qo'sh@@", 'ilgan', 'odam', "bo'ladi", '.']
Pred: ['how', 'do', 'you', 'find', 'out', 'about', 'the', 'control', '?', 'and', 'she', 'was', 'happening', 'as', 'the', 'dancer', 'himself', ',', 'and', 'she', 'was', 'in', 'his', 'age', ',', 'and', 'says', ',', 'then', 'he', 'was', 'happening', 'then', 'she', "'s", 'a', 'lot', 'of', 'school', '.']
Gt: [['yes', ',', 'it', 'was', 'the', 'shirtless', 'guy', 'who', 'was', 'first', ',', 'and', 'he', "'ll", 'get', 'all', 'the', 'credit', ',', 'but', 'it', 'was', 'really', 'the', 'first', 'follower', 'that', 'transformed', 'the', 'lone', 'nut', 'into', 'a', 'leader', '.']]
3) -------------------------- Input: ['u', 'yana', 'bir', 'yil', 'dar@@', 's@@', 'lar', 'olib', 'mash@@', 'q', 'qiladi', ',', 'va', 'endi', 'u', "to'@@", 'q@@', 'qi@@', 'z', 'yosh@@', 'da', '.']
Pred: ['now', ',', 'he', 'practices', 'for', 'a', 'year', 'and', 'takes', 'lessons', '--', 'he', "'s", '10', '.']
Gt: [['he', 'practices', 'for', 'another', 'year', 'and', 'takes', 'lessons', '--', 'he', "'s", 'nine', '.']]
SacreBLEU: BLEU = 19.15 38.3/20.1/16.9/15.7 (BP = 0.901 ratio = 0.905 hyp_len = 3457 ref_len = 3819)
Test Bleu: 19.15 | lr: 0.0011 | Elapsed: 11.8943 min
Epoch: 290 Train loss: 0.9382 acc: 0.7715 | lr: 0.0011 | Elapsed: 11.9227 min
Epoch: 291 Train loss: 0.9371 acc: 0.7719 | lr: 0.0011 | Elapsed: 11.9564 min
Epoch: 292 Train loss: 0.9419 acc: 0.771 | lr: 0.0011 | Elapsed: 11.9901 min
Epoch: 293 Train loss: 0.9382 acc: 0.7725 | lr: 0.0011 | Elapsed: 12.0237 min
Epoch: 294 Train loss: 0.9362 acc: 0.7724 | lr: 0.0011 | Elapsed: 12.0574 min
Epoch: 295 Train loss: 0.9361 acc: 0.7724 | lr: 0.0011 | Elapsed: 12.0911 min
Epoch: 296 Train loss: 0.929 acc: 0.7771 | lr: 0.0011 | Elapsed: 12.1286 min
Epoch: 297 Train loss: 0.9332 acc: 0.7743 | lr: 0.0011 | Elapsed: 12.165 min
Epoch: 298 Train loss: 0.9363 acc: 0.773 | lr: 0.0011 | Elapsed: 12.2036 min
Epoch: 299 Train loss: 0.9315 acc: 0.7743 | Evaluation on Test Set
Evaluation Phase over 193 sentences-----------------------
0) -------------------------- Input: ['chunki', 'dunyo', 'muammo@@', 'lari', 'inson@@', 'larning', 'o@@', 'ila@@', 'viy', 'mer@@', 'o@@', 'si', "bo'lishi", 'kerak', 'emas', '.']
Pred: ['it', "'s", 'not', 'a', 'lot', 'of', 'feedback', 'all', 'of', 'family', '.']
Gt: [['because', 'the', 'world', "'s", 'problems', 'shouldn', "'t", 'be', 'the', 'human', 'family', "'s", 'heirloom', '.']]
1) -------------------------- Input: ['(', '<UNK>', ')']
Pred: ['(', 'laughter', ')']
Gt: [['(', 'applause', ')']]
2) -------------------------- Input: ['li@@', 'der@@', 'lik', ',', 'boshqar@@', 'ish', 'ker@@', 'a@@', 'gi@@', 'dan', 'or@@', 'ti@@', 'q@@', 'cha', 'maq@@', 'ta@@', 'ladi', ',', 'ana', 'u', "ko'@@", 'y@@', 'lak@@', 'siz', 'kishi', 'birinchi', 'bosh@@', 'ladi', 'va', 'hamma', 'sh@@', 'ara@@', 'f', 'unga', 'te@@', 'ga@@', 'di', ',', 'lekin', 'aslida', 'uni', 'li@@', 'der@@', 'ga', 'aylan@@', 'tir@@', 'gan', 'odam', 'ana', 'shu', 'birinchi', "qo'sh@@", 'ilgan', 'odam', "bo'ladi", '.']
Pred: ['but', 'if', 'he', 'was', 'able', 'to', 'do', 'that', ',', 'she', 'was', 'always', 'left', 'with', 'a', 'dancer', ',', 'or', 'when', 'she', 'was', 'in', 'the', 'age', ',', 'received', 'aheat', 'ted', '.', '(', 'laughter', ')']
Gt: [['yes', ',', 'it', 'was', 'the', 'shirtless', 'guy', 'who', 'was', 'first', ',', 'and', 'he', "'ll", 'get', 'all', 'the', 'credit', ',', 'but', 'it', 'was', 'really', 'the', 'first', 'follower', 'that', 'transformed', 'the', 'lone', 'nut', 'into', 'a', 'leader', '.']]
3) -------------------------- Input: ['u', 'yana', 'bir', 'yil', 'dar@@', 's@@', 'lar', 'olib', 'mash@@', 'q', 'qiladi', ',', 'va', 'endi', 'u', "to'@@", 'q@@', 'qi@@', 'z', 'yosh@@', 'da', '.']
Pred: ['now', ',', 'he', 'practices', 'for', 'a', 'year', 'and', 'takes', 'lessons', '--', 'he', "'s", '10', '.']
Gt: [['he', 'practices', 'for', 'another', 'year', 'and', 'takes', 'lessons', '--', 'he', "'s", 'nine', '.']]
SacreBLEU: BLEU = 18.63 40.4/21.1/17.8/16.7 (BP = 0.830 ratio = 0.843 hyp_len = 3218 ref_len = 3819)
Test Bleu: 18.63 | lr: 0.0011 | Elapsed: 12.365 min
Traceback (most recent call last):



# # # # # # # # # # # # # # # # # # # # # # # # # #  ## # # # # # #  # # #

python3.7 train_USKI.py --selected_model transformer --N 3 \
    --dropout 0.15 --seed 1234  --num_epochs 320 \
    --max_len 250  --min_len 2 \
    --d_model 128 \
    --sched_type noam --warmup_steps 400 \
    --batch_size 2048 --num_accum 2 \
    --pretrain_batch_size 128 --pretrain_num_accum 2 \
    --num_pretrain_iter 42000 \
    --eval_beam_size 4 \
    --eval_batch_size 32 \
    --save_path ./github_ignore_material/saves/ \
    --language uz-en_4000 --num_gpus 1 \
    --ddp_sync_port 12139 &> output.txt &

Trg : [1, 617, 5, 2]
1) --------------------------------------------
Src : [1, 844, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 2]
Trg : [1, 520, 734, 2821, 153, 243, 2945, 2946, 2947, 616, 2948, 699, 1088, 2682, 10, 2949, 2950, 1822, 19, 21, 105, 3916, 107, 3131, 2952, 2953, 2092, 2954, 1116, 2917, 2853, 2652, 2955, 243, 2956, 136, 65, 2363, 2957, 2092, 21, 105, 3916, 107, 2]
Len: 3539
./bpe_data/qed_uz_en/4000_train.uz read 3689 sentences
./bpe_data/qed_uz_en/4000_train.en read 3689 sentences
./bpe_data/qed_uz_en/4000_val.uz read 99 sentences
./bpe_data/qed_uz_en/4000_val.en read 99 sentences
Using -  1  processes / GPUs!
Requested num GPUs: 1
GPU: 0] Process 0 working...
Creating pretraining - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
<class 'model.transformer.TransformerMT'>
File not found --- FIRST Pretraining
Pre-Training...
Train epoch: 0 - it 500 / 13601 final loss: 4.168949 pre-train val: 3.243916
Train epoch: 0 - it 1000 / 13601 final loss: 3.330828 pre-train val: 2.396602
Train epoch: 0 - it 1500 / 13601 final loss: 2.936645 pre-train val: 2.153731
Train epoch: 0 - it 2000 / 13601 final loss: 2.705316 pre-train val: 2.01903
Train epoch: 0 - it 2500 / 13601 final loss: 2.553525 pre-train val: 1.945281
Train epoch: 0 - it 3000 / 13601 final loss: 2.445268 pre-train val: 1.895176
Train epoch: 0 - it 3500 / 13601 final loss: 2.362973 pre-train val: 1.828135
Train epoch: 0 - it 4000 / 13601 final loss: 2.298174 pre-train val: 1.798736
Train epoch: 0 - it 4500 / 13601 final loss: 2.245682 pre-train val: 1.771361
Train epoch: 0 - it 5000 / 13601 final loss: 2.202088 pre-train val: 1.774722
Train epoch: 0 - it 5500 / 13601 final loss: 2.165465 pre-train val: 1.747172
Train epoch: 0 - it 6000 / 13601 final loss: 2.134219 pre-train val: 1.726623
Train epoch: 0 - it 6500 / 13601 final loss: 2.106961 pre-train val: 1.718251
Train epoch: 0 - it 7000 / 13601 final loss: 2.083348 pre-train val: 1.707635
Train epoch: 0 - it 7500 / 13601 final loss: 2.062173 pre-train val: 1.694217
Train epoch: 0 - it 8000 / 13601 final loss: 2.043562 pre-train val: 1.70088
 Train epoch: 0 - it 8500 / 13601 final loss: 2.027089 pre-train val: 1.685288
 Train epoch: 0 - it 9000 / 13601 final loss: 2.012228 pre-train val: 1.684988
 Train epoch: 0 - it 9500 / 13601 final loss: 1.99858 pre-train val: 1.681579
 Train epoch: 0 - it 10000 / 13601 final loss: 1.985997 pre-train val: 1.673352
 Train epoch: 0 - it 10500 / 13601 final loss: 1.974731 pre-train val: 1.664589
 Train epoch: 0 - it 11000 / 13601 final loss: 1.964373 pre-train val: 1.66317
 Train epoch: 0 - it 11500 / 13601 final loss: 1.954658 pre-train val: 1.656535
 Train epoch: 0 - it 12000 / 13601 final loss: 1.945765 pre-train val: 1.658685
 Train epoch: 0 - it 12500 / 13601 final loss: 1.937802 pre-train val: 1.651512
 Train epoch: 0 - it 13000 / 13601 final loss: 1.930141 pre-train val: 1.658218
 Train epoch: 0 - it 13500 / 13601 final loss: 1.922908 pre-train val: 1.656521
 Train epoch: 1 - it 14101 / 13601 final loss: 1.91496 pre-train val: 1.65367
 Train epoch: 1 - it 14601 / 13601 final loss: 1.90874 pre-train val: 1.645143
 Train epoch: 1 - it 15101 / 13601 final loss: 1.902829 pre-train val: 1.649558
 Train epoch: 1 - it 15601 / 13601 final loss: 1.8973 pre-train val: 1.641484
 Train epoch: 1 - it 16101 / 13601 final loss: 1.892108 pre-train val: 1.64638
 Train epoch: 1 - it 16601 / 13601 final loss: 1.887253 pre-train val: 1.641289
 Train epoch: 1 - it 17101 / 13601 final loss: 1.882581 pre-train val: 1.63874
 Train epoch: 1 - it 17601 / 13601 final loss: 1.878254 pre-train val: 1.639695
 Train epoch: 1 - it 18101 / 13601 final loss: 1.874072 pre-train val: 1.637208
 Train epoch: 1 - it 18601 / 13601 final loss: 1.870111 pre-train val: 1.63667
 Train epoch: 1 - it 19101 / 13601 final loss: 1.866332 pre-train val: 1.638561
 Train epoch: 1 - it 19601 / 13601 final loss: 1.86269 pre-train val: 1.632361
 Saved checkpoint: ./github_ignore_material/saves/pretrained_model.pt
Train epoch: 1 - it 20101 / 13601 final loss: 1.859208 pre-train val: 1.633309
Train epoch: 1 - it 20601 / 13601 final loss: 1.855952 pre-train val: 1.630895
Train epoch: 1 - it 21101 / 13601 final loss: 1.852738 pre-train val: 1.628442
Train epoch: 1 - it 21601 / 13601 final loss: 1.849739 pre-train val: 1.626777
Train epoch: 1 - it 22101 / 13601 final loss: 1.846924 pre-train val: 1.626493
Train epoch: 1 - it 22601 / 13601 final loss: 1.844214 pre-train val: 1.631479
Train epoch: 1 - it 23101 / 13601 final loss: 1.841562 pre-train val: 1.628443
Train epoch: 1 - it 23601 / 13601 final loss: 1.838885 pre-train val: 1.624717
Train epoch: 1 - it 24101 / 13601 final loss: 1.836427 pre-train val: 1.6267
Train epoch: 1 - it 24601 / 13601 final loss: 1.834059 pre-train val: 1.624514
Train epoch: 1 - it 25101 / 13601 final loss: 1.831666 pre-train val: 1.621331
Train epoch: 1 - it 25601 / 13601 final loss: 1.82946 pre-train val: 1.618545
Train epoch: 1 - it 26101 / 13601 final loss: 1.827434 pre-train val: 1.620845
Train epoch: 1 - it 26601 / 13601 final loss: 1.825384 pre-train val: 1.626056
Train epoch: 1 - it 27101 / 13601 final loss: 1.823316 pre-train val: 1.62257
Train epoch: 2 - it 27702 / 13601 final loss: 1.82099 pre-train val: 1.622499
Train epoch: 2 - it 28202 / 13601 final loss: 1.819118 pre-train val: 1.618416
Train epoch: 2 - it 28702 / 13601 final loss: 1.81727 pre-train val: 1.618121
Train epoch: 2 - it 29202 / 13601 final loss: 1.815503 pre-train val: 1.618502
Train epoch: 2 - it 29702 / 13601 final loss: 1.813793 pre-train val: 1.618135
Train epoch: 2 - it 30202 / 13601 final loss: 1.812173 pre-train val: 1.615442
Train epoch: 2 - it 30702 / 13601 final loss: 1.810559 pre-train val: 1.616418
Train epoch: 2 - it 31202 / 13601 final loss: 1.809049 pre-train val: 1.617067
Train epoch: 2 - it 31702 / 13601 final loss: 1.807532 pre-train val: 1.61459
Train epoch: 2 - it 32202 / 13601 final loss: 1.806064 pre-train val: 1.616069
Train epoch: 2 - it 32702 / 13601 final loss: 1.804636 pre-train val: 1.616902
Train epoch: 2 - it 33202 / 13601 final loss: 1.803219 pre-train val: 1.614519
Train epoch: 2 - it 33702 / 13601 final loss: 1.801856 pre-train val: 1.61417
Train epoch: 2 - it 34202 / 13601 final loss: 1.800548 pre-train val: 1.614929
Train epoch: 2 - it 34702 / 13601 final loss: 1.799222 pre-train val: 1.612056
Train epoch: 2 - it 35202 / 13601 final loss: 1.798002 pre-train val: 1.611155
Train epoch: 2 - it 35702 / 13601 final loss: 1.796841 pre-train val: 1.6111
Train epoch: 2 - it 36202 / 13601 final loss: 1.795675 pre-train val: 1.611942
Train epoch: 2 - it 36702 / 13601 final loss: 1.794524 pre-train val: 1.610545
Train epoch: 2 - it 37202 / 13601 final loss: 1.793333 pre-train val: 1.611696
Train epoch: 2 - it 37702 / 13601 final loss: 1.792231 pre-train val: 1.610111
Train epoch: 2 - it 38202 / 13601 final loss: 1.791162 pre-train val: 1.609541
Train epoch: 2 - it 38702 / 13601 final loss: 1.790038 pre-train val: 1.60763
Train epoch: 2 - it 39202 / 13601 final loss: 1.789008 pre-train val: 1.608439
Train epoch: 2 - it 39702 / 13601 final loss: 1.788077 pre-train val: 1.608798
Saved checkpoint: ./github_ignore_material/saves/pretrained_model.pt
Train epoch: 2 - it 40202 / 13601 final loss: 1.787118 pre-train val: 1.612108
Train epoch: 2 - it 40702 / 13601 final loss: 1.786119 pre-train val: 1.60788
Train epoch: 3 - it 41303 / 13601 final loss: 1.784994 pre-train val: 1.610555
Train epoch: 3 - it 41803 / 13601 final loss: 1.784083 pre-train val: 1.60937
Reached early step iter, exiting pre-training
Reached early step iter, exiting pre-training
First pretraing save -- Done.
Classification model - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
Mapped pre-trained weights to Classification model
Start NMT Training post Pre-Training...
Loading: no checkpoint found in ./github_ignore_material/saves/
How many batches: 44
Epoch: 0 Train loss: 9.6487 acc: 0.0015
lr: 0.0002 | Elapsed: 0.0408 min
Epoch: 1 Train loss: 6.786 acc: 0.0128
lr: 0.0005 | Elapsed: 0.0795 min
Epoch: 2 Train loss: 5.91 acc: 0.0229
lr: 0.0007 | Elapsed: 0.1177 min
Epoch: 3 Train loss: 5.4628 acc: 0.0393
lr: 0.001 | Elapsed: 0.1559 min
Epoch: 4 Train loss: 5.1531 acc: 0.0865
lr: 0.0012 | Elapsed: 0.1948 min
Epoch: 5 Train loss: 4.9429 acc: 0.1115
lr: 0.0015 | Elapsed: 0.2333 min
Epoch: 6 Train loss: 4.7429 acc: 0.1322
lr: 0.0017 | Elapsed: 0.272 min
Epoch: 7 Train loss: 4.5266 acc: 0.15
lr: 0.0019 | Elapsed: 0.3096 min
Epoch: 8 Train loss: 4.3063 acc: 0.1705
lr: 0.0022 | Elapsed: 0.3451 min
Epoch: 9 Train loss: 4.1035 acc: 0.1917
Val loss: 4.2063 acc: 0.2578 | Evaluation on Test Set

...
Evaluation Phase over 193 sentences-----------------------
0) -------------------------- Input: ['chunki', 'dunyo', 'muammo@@', 'lari', 'inson@@', 'larning', 'o@@', 'ila@@', 'viy', 'mer@@', 'o@@', 'si', "bo'lishi", 'kerak', 'emas', '.']
Pred: ['the', 'universe', 'of', 'the', 'value', 'of', 'the', 'expreciself-', 'doubt', '.']
Gt: [['because', 'the', 'world', "'s", 'problems', 'shouldn', "'t", 'be', 'the', 'human', 'family', "'s", 'heirloom', '.']]
1) -------------------------- Input: ['(', '<UNK>', ')']
Pred: ['(', '<UNK>', ')']
Gt: [['(', 'applause', ')']]
2) -------------------------- Input: ['li@@', 'der@@', 'lik', ',', 'boshqar@@', 'ish', 'ker@@', 'a@@', 'gi@@', 'dan', 'or@@', 'ti@@', 'q@@', 'cha', 'maq@@', 'ta@@', 'ladi', ',', 'ana', 'u', "ko'@@", 'y@@', 'lak@@', 'siz', 'kishi', 'birinchi', 'bosh@@', 'ladi', 'va', 'hamma', 'sh@@', 'ara@@', 'f', 'unga', 'te@@', 'ga@@', 'di', ',', 'lekin', 'aslida', 'uni', 'li@@', 'der@@', 'ga', 'aylan@@', 'tir@@', 'gan', 'odam', 'ana', 'shu', 'birinchi', "qo'sh@@", 'ilgan', 'odam', "bo'ladi", '.']
Pred: ['the', 'first', 'thing', 'that', 'when', 'you', 'give', 'up', 'to', 'the', 'first', 'time', ',', 'to', 'be', 'honest', 'and', 'to', 'the', 'first', 'time', ',', 'but', 'to', 'be', 'the', 'first', 'generation', 'in', 'the', 'world', '.']
Gt: [['yes', ',', 'it', 'was', 'the', 'shirtless', 'guy', 'who', 'was', 'first', ',', 'and', 'he', "'ll", 'get', 'all', 'the', 'credit', ',', 'but', 'it', 'was', 'really', 'the', 'first', 'follower', 'that', 'transformed', 'the', 'lone', 'nut', 'into', 'a', 'leader', '.']]
3) -------------------------- Input: ['u', 'yana', 'bir', 'yil', 'dar@@', 's@@', 'lar', 'olib', 'mash@@', 'q', 'qiladi', ',', 'va', 'endi', 'u', "to'@@", 'q@@', 'qi@@', 'z', 'yosh@@', 'da', '.']
Pred: ['it', "'s", 'the', 'wrong', 'chord', 'with', 'the', 'same', 'patterns', ',', 'and', 'he', "'s", 'the', 'wrong', '.']
Gt: [['he', 'practices', 'for', 'another', 'year', 'and', 'takes', 'lessons', '--', 'he', "'s", 'nine', '.']]
SacreBLEU: BLEU = 18.38 37.3/18.9/15.2/14.1 (BP = 0.932 ratio = 0.935 hyp_len = 3569 ref_len = 3819)
Test Bleu: 18.38 | lr: 0.0014 | Elapsed: 8.9055 min
Epoch: 170 Train loss: 1.0881 acc: 0.727  lr: 0.0014 | Elapsed: 8.9449 min
Epoch: 171 Train loss: 1.0822 acc: 0.7295 lr: 0.0014 | Elapsed: 8.9801 min
Epoch: 172 Train loss: 1.0785 acc: 0.7299 lr: 0.0014 | Elapsed: 9.016 min
Epoch: 173 Train loss: 1.074 acc: 0.732 lr: 0.0014 | Elapsed: 9.0535 min
Epoch: 174 Train loss: 1.0754 acc: 0.7335 lr: 0.0014 | Elapsed: 9.0913 min
Epoch: 175 Train loss: 1.0703 acc: 0.732 lr: 0.0014 | Elapsed: 9.1285 min
Epoch: 176 Train loss: 1.0726 acc: 0.732 lr: 0.0014 | Elapsed: 9.1659 min
Epoch: 177 Train loss: 1.0685 acc: 0.7351 lr: 0.0014 | Elapsed: 9.2037 min
Epoch: 178 Train loss: 1.0679 acc: 0.7348 lr: 0.0014 | Elapsed: 9.2416 min
Epoch: 179 Train loss: 1.059 acc: 0.7354
Val loss: 4.1373 acc: 0.4299 | Evaluation on Test Set
Evaluation Phase over 193 sentences-----------------------
0) -------------------------- Input: ['chunki', 'dunyo', 'muammo@@', 'lari', 'inson@@', 'larning', 'o@@', 'ila@@', 'viy', 'mer@@', 'o@@', 'si', "bo'lishi", 'kerak', 'emas', '.']
Pred: ['the', 'universe', 'of', 'the', 'value', 'of', 'people', '.']
Gt: [['because', 'the', 'world', "'s", 'problems', 'shouldn', "'t", 'be', 'the', 'human', 'family', "'s", 'heirloom', '.']]
1) -------------------------- Input: ['(', '<UNK>', ')']
Pred: ['(', '<UNK>', ')']
Gt: [['(', 'applause', ')']]
2) -------------------------- Input: ['li@@', 'der@@', 'lik', ',', 'boshqar@@', 'ish', 'ker@@', 'a@@', 'gi@@', 'dan', 'or@@', 'ti@@', 'q@@', 'cha', 'maq@@', 'ta@@', 'ladi', ',', 'ana', 'u', "ko'@@", 'y@@', 'lak@@', 'siz', 'kishi', 'birinchi', 'bosh@@', 'ladi', 'va', 'hamma', 'sh@@', 'ara@@', 'f', 'unga', 'te@@', 'ga@@', 'di', ',', 'lekin', 'aslida', 'uni', 'li@@', 'der@@', 'ga', 'aylan@@', 'tir@@', 'gan', 'odam', 'ana', 'shu', 'birinchi', "qo'sh@@", 'ilgan', 'odam', "bo'ladi", '.']
Pred: ['whether', 'pattered', 'by', 'the', 'plane', ',', 'when', 'he', 'wd', 'to', 'the', 'leader', ',', 'when', 'he', 'wine', 'again', 'for', 'the', 'first', 'generation', ',', 'with', 'the', 'world', '.']
Gt: [['yes', ',', 'it', 'was', 'the', 'shirtless', 'guy', 'who', 'was', 'first', ',', 'and', 'he', "'ll", 'get', 'all', 'the', 'credit', ',', 'but', 'it', 'was', 'really', 'the', 'first', 'follower', 'that', 'transformed', 'the', 'lone', 'nut', 'into', 'a', 'leader', '.']]
3) -------------------------- Input: ['u', 'yana', 'bir', 'yil', 'dar@@', 's@@', 'lar', 'olib', 'mash@@', 'q', 'qiladi', ',', 'va', 'endi', 'u', "to'@@", 'q@@', 'qi@@', 'z', 'yosh@@', 'da', '.']
Pred: ['he', 'practices', 'for', 'another', 'year', 'and', 'takes', 'lessons', '--', 'and', 'he', "'s", 'the', 'same', 'warm', '.']
Gt: [['he', 'practices', 'for', 'another', 'year', 'and', 'takes', 'lessons', '--', 'he', "'s", 'nine', '.']]
SacreBLEU: BLEU = 17.67 35.7/17.2/13.6/12.4 (BP = 0.985 ratio = 0.985 hyp_len = 3761 ref_len = 3819)
Test Bleu: 17.67 | lr: 0.0014 | Elapsed: 9.42 min
Epoch: 180 Train loss: 1.056 acc: 0.7369 lr: 0.0014 | Elapsed: 9.4532 min
Epoch: 181 Train loss: 1.0595 acc: 0.7351 lr: 0.0014 | Elapsed: 9.4906 min
Epoch: 182 Train loss: 1.051 acc: 0.7387 lr: 0.0014 | Elapsed: 9.5282 min
Epoch: 183 Train loss: 1.0498 acc: 0.7381 lr: 0.0014 | Elapsed: 9.5653 min
Epoch: 184 Train loss: 1.0456 acc: 0.7407 lr: 0.0014 | Elapsed: 9.6024 min
Epoch: 185 Train loss: 1.0424 acc: 0.7417 lr: 0.0014 | Elapsed: 9.6402 min
Epoch: 186 Train loss: 1.0478 acc: 0.7397 lr: 0.0014 | Elapsed: 9.6787 min
Epoch: 187 Train loss: 1.0414 acc: 0.7394 lr: 0.0014 | Elapsed: 9.7157 min
Epoch: 188 Train loss: 1.044 acc: 0.7419 lr: 0.0014 | Elapsed: 9.7535 min
Epoch: 189 Train loss: 1.0437 acc: 0.7401
Val loss: 4.1452 acc: 0.4426 | Evaluation on Test Set
Evaluation Phase over 193 sentences-----------------------
0) -------------------------- Input: ['chunki', 'dunyo', 'muammo@@', 'lari', 'inson@@', 'larning', 'o@@', 'ila@@', 'viy', 'mer@@', 'o@@', 'si', "bo'lishi", 'kerak', 'emas', '.']
Pred: ['so', 'there', "'s", 'a', 'great', 'problem', '.']
Gt: [['because', 'the', 'world', "'s", 'problems', 'shouldn', "'t", 'be', 'the', 'human', 'family', "'s", 'heirloom', '.']]
1) -------------------------- Input: ['(', '<UNK>', ')']
Pred: ['(', '<UNK>', ')']
Gt: [['(', 'applause', ')']]
2) -------------------------- Input: ['li@@', 'der@@', 'lik', ',', 'boshqar@@', 'ish', 'ker@@', 'a@@', 'gi@@', 'dan', 'or@@', 'ti@@', 'q@@', 'cha', 'maq@@', 'ta@@', 'ladi', ',', 'ana', 'u', "ko'@@", 'y@@', 'lak@@', 'siz', 'kishi', 'birinchi', 'bosh@@', 'ladi', 'va', 'hamma', 'sh@@', 'ara@@', 'f', 'unga', 'te@@', 'ga@@', 'di', ',', 'lekin', 'aslida', 'uni', 'li@@', 'der@@', 'ga', 'aylan@@', 'tir@@', 'gan', 'odam', 'ana', 'shu', 'birinchi', "qo'sh@@", 'ilgan', 'odam', "bo'ladi", '.']
Pred: ['wrote', 'this', 'to', 'the', 'first', 'time', ',', 'in', 'the', 'west', 'wall', 'the', 'wrisky', ',', 'and', 'the', 'first', 'generation', ',', 'to', 'be', 'able', 'to', 'train', 'the', 'world', '.']
Gt: [['yes', ',', 'it', 'was', 'the', 'shirtless', 'guy', 'who', 'was', 'first', ',', 'and', 'he', "'ll", 'get', 'all', 'the', 'credit', ',', 'but', 'it', 'was', 'really', 'the', 'first', 'follower', 'that', 'transformed', 'the', 'lone', 'nut', 'into', 'a', 'leader', '.']]
3) -------------------------- Input: ['u', 'yana', 'bir', 'yil', 'dar@@', 's@@', 'lar', 'olib', 'mash@@', 'q', 'qiladi', ',', 'va', 'endi', 'u', "to'@@", 'q@@', 'qi@@', 'z', 'yosh@@', 'da', '.']
Pred: ['it', "'s", 'the', 'wrong', 'with', 'the', 'same', 'time', ',', 'and', 'he', "'s", 'the', 'same', 'wards', '.']
Gt: [['he', 'practices', 'for', 'another', 'year', 'and', 'takes', 'lessons', '--', 'he', "'s", 'nine', '.']]
SacreBLEU: BLEU = 18.76 38.9/19.9/16.4/15.2 (BP = 0.895 ratio = 0.900 hyp_len = 3438 ref_len = 3819)
Test Bleu: 18.76 | lr: 0.0014 | Elapsed: 9.9184 min
Epoch: 190 Train loss: 1.0394 acc: 0.7415 lr: 0.0014 | Elapsed: 9.9521 min
Epoch: 191 Train loss: 1.0368 acc: 0.742  lr: 0.0014 | Elapsed: 9.9897 min
Epoch: 192 Train loss: 1.0389 acc: 0.7408 lr: 0.0014 | Elapsed: 10.0268 min
Epoch: 193 Train loss: 1.0373 acc: 0.7424 lr: 0.0014 | Elapsed: 10.0648 min
Epoch: 194 Train loss: 1.0294 acc: 0.745 lr: 0.0013 | Elapsed: 10.1024 min
Epoch: 195 Train loss: 1.0296 acc: 0.7456 lr: 0.0013 | Elapsed: 10.1403 min
Epoch: 196 Train loss: 1.0262 acc: 0.7445 lr: 0.0013 | Elapsed: 10.1779 min
Epoch: 197 Train loss: 1.0262 acc: 0.7465 lr: 0.0013 | Elapsed: 10.2153 min
Epoch: 198 Train loss: 1.021 acc: 0.7473 lr: 0.0013 | Elapsed: 10.2529 min
Epoch: 199 Train loss: 1.0179 acc: 0.748
Val loss: 4.131 acc: 0.435 | Evaluation on Test Set
Evaluation Phase over 193 sentences-----------------------
0) -------------------------- Input: ['chunki', 'dunyo', 'muammo@@', 'lari', 'inson@@', 'larning', 'o@@', 'ila@@', 'viy', 'mer@@', 'o@@', 'si', "bo'lishi", 'kerak', 'emas', '.']
Pred: ['so', 'there', "'s", 'a', 'problem', 'for', 'the', 'second', '.']
Gt: [['because', 'the', 'world', "'s", 'problems', 'shouldn', "'t", 'be', 'the', 'human', 'family', "'s", 'heirloom', '.']]
1) -------------------------- Input: ['(', '<UNK>', ')']
Pred: ['(', '<UNK>', ')']
Gt: [['(', 'applause', ')']]
2) -------------------------- Input: ['li@@', 'der@@', 'lik', ',', 'boshqar@@', 'ish', 'ker@@', 'a@@', 'gi@@', 'dan', 'or@@', 'ti@@', 'q@@', 'cha', 'maq@@', 'ta@@', 'ladi', ',', 'ana', 'u', "ko'@@", 'y@@', 'lak@@', 'siz', 'kishi', 'birinchi', 'bosh@@', 'ladi', 'va', 'hamma', 'sh@@', 'ara@@', 'f', 'unga', 'te@@', 'ga@@', 'di', ',', 'lekin', 'aslida', 'uni', 'li@@', 'der@@', 'ga', 'aylan@@', 'tir@@', 'gan', 'odam', 'ana', 'shu', 'birinchi', "qo'sh@@", 'ilgan', 'odam', "bo'ladi", '.']
Pred: ['whether', 'patterns', ',', 'he', 'said', 'that', 'when', 'you', 'look', 'at', 'the', 'first', 'time', ',', 'he', 'wised', 'to', 'the', 'first', 'generation', ',', 'but', 'the', 'first', 'following', 'up', 'to', 'the', 'leader', '.']
Gt: [['yes', ',', 'it', 'was', 'the', 'shirtless', 'guy', 'who', 'was', 'first', ',', 'and', 'he', "'ll", 'get', 'all', 'the', 'credit', ',', 'but', 'it', 'was', 'really', 'the', 'first', 'follower', 'that', 'transformed', 'the', 'lone', 'nut', 'into', 'a', 'leader', '.']]
3) -------------------------- Input: ['u', 'yana', 'bir', 'yil', 'dar@@', 's@@', 'lar', 'olib', 'mash@@', 'q', 'qiladi', ',', 'va', 'endi', 'u', "to'@@", 'q@@', 'qi@@', 'z', 'yosh@@', 'da', '.']
Pred: ['it', "'s", 'the', 'wrong', 'chord', 'for', 'the', 'same', 'thing', ',', 'and', 'she', "'s", 'the', 'wrong', '.']
Gt: [['he', 'practices', 'for', 'another', 'year', 'and', 'takes', 'lessons', '--', 'he', "'s", 'nine', '.']]
SacreBLEU: BLEU = 19.16 38.8/20.1/16.7/15.9 (BP = 0.898 ratio = 0.903 hyp_len = 3448 ref_len = 3819)
Test Bleu: 19.16 | lr: 0.0013 | Elapsed: 10.4251 min
Epoch: 200 Train loss: 1.0183 acc: 0.7496 lr: 0.0013 | Elapsed: 10.4584 min
Epoch: 201 Train loss: 1.0174 acc: 0.7477 lr: 0.0013 | Elapsed: 10.4961 min
Epoch: 202 Train loss: 1.0114 acc: 0.7506 lr: 0.0013 | Elapsed: 10.5336 min
Epoch: 203 Train loss: 1.0086 acc: 0.7526 lr: 0.0013 | Elapsed: 10.571 min
Epoch: 204 Train loss: 1.0081 acc: 0.7504 lr: 0.0013 | Elapsed: 10.6088 min
Epoch: 205 Train loss: 1.0071 acc: 0.7511 lr: 0.0013 | Elapsed: 10.6466 min
Epoch: 206 Train loss: 1.0041 acc: 0.7524 lr: 0.0013 | Elapsed: 10.6841 min
Epoch: 207 Train loss: 0.9995 acc: 0.7545  lr: 0.0013 | Elapsed: 10.7221 min
Epoch: 208 Train loss: 1.0011 acc: 0.7531 lr: 0.0013 | Elapsed: 10.76 min
Epoch: 209 Train loss: 0.9994 acc: 0.7537
Val loss: 4.1587 acc: 0.4419 | Evaluation on Test Set
Evaluation Phase over 193 sentences-----------------------
0) -------------------------- Input: ['chunki', 'dunyo', 'muammo@@', 'lari', 'inson@@', 'larning', 'o@@', 'ila@@', 'viy', 'mer@@', 'o@@', 'si', "bo'lishi", 'kerak', 'emas', '.']
Pred: ['because', 'the', 'universe', 'of', 'the', 'value', 'of', 'the', 'expreciated', '.']
Gt: [['because', 'the', 'world', "'s", 'problems', 'shouldn', "'t", 'be', 'the', 'human', 'family', "'s", 'heirloom', '.']]
1) -------------------------- Input: ['(', '<UNK>', ')']
Pred: ['(', '<UNK>', ')']
Gt: [['(', 'applause', ')']]
2) -------------------------- Input: ['li@@', 'der@@', 'lik', ',', 'boshqar@@', 'ish', 'ker@@', 'a@@', 'gi@@', 'dan', 'or@@', 'ti@@', 'q@@', 'cha', 'maq@@', 'ta@@', 'ladi', ',', 'ana', 'u', "ko'@@", 'y@@', 'lak@@', 'siz', 'kishi', 'birinchi', 'bosh@@', 'ladi', 'va', 'hamma', 'sh@@', 'ara@@', 'f', 'unga', 'te@@', 'ga@@', 'di', ',', 'lekin', 'aslida', 'uni', 'li@@', 'der@@', 'ga', 'aylan@@', 'tir@@', 'gan', 'odam', 'ana', 'shu', 'birinchi', "qo'sh@@", 'ilgan', 'odam', "bo'ladi", '.']
Pred: ['whether', 'patterns', ',', 'he', 'said', 'this', 'drive', 'the', 'tewin', 'the', 'first', 'time', ',', 'and', 'the', 'first', 'generation', ',', 'which', 'spans', ',', 'but', 'the', 'first', 'generation', 'again', '.']
Gt: [['yes', ',', 'it', 'was', 'the', 'shirtless', 'guy', 'who', 'was', 'first', ',', 'and', 'he', "'ll", 'get', 'all', 'the', 'credit', ',', 'but', 'it', 'was', 'really', 'the', 'first', 'follower', 'that', 'transformed', 'the', 'lone', 'nut', 'into', 'a', 'leader', '.']]
3) -------------------------- Input: ['u', 'yana', 'bir', 'yil', 'dar@@', 's@@', 'lar', 'olib', 'mash@@', 'q', 'qiladi', ',', 'va', 'endi', 'u', "to'@@", 'q@@', 'qi@@', 'z', 'yosh@@', 'da', '.']
Pred: ['it', "'s", 'the', 'same', 'wards', 'for', 'the', 'same', 'thing', 'syringe', 'and', 'he', "'s", 'the', 'same', 'warm', '.']
Gt: [['he', 'practices', 'for', 'another', 'year', 'and', 'takes', 'lessons', '--', 'he', "'s", 'nine', '.']]
SacreBLEU: BLEU = 19.71 40.5/21.0/17.8/16.9 (BP = 0.876 ratio = 0.883 hyp_len = 3373 ref_len = 3819)
Test Bleu: 19.71 | lr: 0.0013 | Elapsed: 10.9338 min
Epoch: 210 Train loss: 0.9933 acc: 0.7559 lr: 0.0013 | Elapsed: 10.9682 min
Epoch: 211 Train loss: 0.9915 acc: 0.7554 lr: 0.0013 | Elapsed: 11.007 min
Epoch: 212 Train loss: 0.9947 acc: 0.7559 lr: 0.0013 | Elapsed: 11.0448 min
Epoch: 213 Train loss: 0.995 acc: 0.7558 lr: 0.0013 | Elapsed: 11.0827 min
Epoch: 214 Train loss: 0.9931 acc: 0.7547 lr: 0.0013 | Elapsed: 11.1214 min
Epoch: 215 Train loss: 0.9936 acc: 0.754 lr: 0.0013 | Elapsed: 11.1597 min
Epoch: 216 Train loss: 0.991 acc: 0.7572 lr: 0.0013 | Elapsed: 11.1979 min
Epoch: 217 Train loss: 0.9842 acc: 0.7581 lr: 0.0013 | Elapsed: 11.2364 min
Epoch: 218 Train loss: 0.9811 acc: 0.7597 lr: 0.0013 | Elapsed: 11.2736 min
Epoch: 219 Train loss: 0.9751 acc: 0.7609
Val loss: 4.1499 acc: 0.4499 | Evaluation on Test Set
Evaluation Phase over 193 sentences-----------------------
0) -------------------------- Input: ['chunki', 'dunyo', 'muammo@@', 'lari', 'inson@@', 'larning', 'o@@', 'ila@@', 'viy', 'mer@@', 'o@@', 'si', "bo'lishi", 'kerak', 'emas', '.']
Pred: ['because', 'the', 'universe', 'of', 'intelligence', 'is', 'not', '.']
Gt: [['because', 'the', 'world', "'s", 'problems', 'shouldn', "'t", 'be', 'the', 'human', 'family', "'s", 'heirloom', '.']]
1) -------------------------- Input: ['(', '<UNK>', ')']
Pred: ['(', 'laughter', ')']
Gt: [['(', 'applause', ')']]
2) -------------------------- Input: ['li@@', 'der@@', 'lik', ',', 'boshqar@@', 'ish', 'ker@@', 'a@@', 'gi@@', 'dan', 'or@@', 'ti@@', 'q@@', 'cha', 'maq@@', 'ta@@', 'ladi', ',', 'ana', 'u', "ko'@@", 'y@@', 'lak@@', 'siz', 'kishi', 'birinchi', 'bosh@@', 'ladi', 'va', 'hamma', 'sh@@', 'ara@@', 'f', 'unga', 'te@@', 'ga@@', 'di', ',', 'lekin', 'aslida', 'uni', 'li@@', 'der@@', 'ga', 'aylan@@', 'tir@@', 'gan', 'odam', 'ana', 'shu', 'birinchi', "qo'sh@@", 'ilgan', 'odam', "bo'ladi", '.']
Pred: ['wrote', 'this', 'plain', 'the', 'first', 'time', ',', 'when', 'he', 'west', 'for', 'the', 'first', 'time', ',', 'the', 'first', 'time', 'that', 'the', 'first', 'generation', '.']
Gt: [['yes', ',', 'it', 'was', 'the', 'shirtless', 'guy', 'who', 'was', 'first', ',', 'and', 'he', "'ll", 'get', 'all', 'the', 'credit', ',', 'but', 'it', 'was', 'really', 'the', 'first', 'follower', 'that', 'transformed', 'the', 'lone', 'nut', 'into', 'a', 'leader', '.']]
3) -------------------------- Input: ['u', 'yana', 'bir', 'yil', 'dar@@', 's@@', 'lar', 'olib', 'mash@@', 'q', 'qiladi', ',', 'va', 'endi', 'u', "to'@@", 'q@@', 'qi@@', 'z', 'yosh@@', 'da', '.']
Pred: ['it', "'s", 'the', 'wrong', 'with', 'the', 'same', 'breakes', ',', 'and', 'he', "'s", 'the', 'wrong', '.']
Gt: [['he', 'practices', 'for', 'another', 'year', 'and', 'takes', 'lessons', '--', 'he', "'s", 'nine', '.']]
SacreBLEU: BLEU = 19.77 39.5/20.5/16.9/16.0 (BP = 0.915 ratio = 0.918 hyp_len = 3507 ref_len = 3819)
Test Bleu: 19.77 | lr: 0.0013 | Elapsed: 11.4395 min
Epoch: 220 Train loss: 0.978 acc: 0.7602 lr: 0.0013 | Elapsed: 11.4749 min
Epoch: 221 Train loss: 0.9761 acc: 0.7602 lr: 0.0013 | Elapsed: 11.5129 min
Epoch: 222 Train loss: 0.9765 acc: 0.7595 lr: 0.0013 | Elapsed: 11.5505 min
Epoch: 223 Train loss: 0.9733 acc: 0.76 lr: 0.0013 | Elapsed: 11.5879 min
Epoch: 224 Train loss: 0.9754 acc: 0.7611 lr: 0.0013 | Elapsed: 11.626 min
Epoch: 225 Train loss: 0.9687 acc: 0.763 lr: 0.0013 | Elapsed: 11.6641 min
Epoch: 226 Train loss: 0.9671 acc: 0.7639 lr: 0.0013 | Elapsed: 11.7015 min
Epoch: 227 Train loss: 0.975 acc: 0.7605 lr: 0.0012 | Elapsed: 11.7394 min
Epoch: 228 Train loss: 0.9639 acc: 0.7649 lr: 0.0012 | Elapsed: 11.7772 min
Epoch: 229 Train loss: 0.9683 acc: 0.7643
Val loss: 4.161 acc: 0.4304 | Evaluation on Test Set
Evaluation Phase over 193 sentences-----------------------
0) -------------------------- Input: ['chunki', 'dunyo', 'muammo@@', 'lari', 'inson@@', 'larning', 'o@@', 'ila@@', 'viy', 'mer@@', 'o@@', 'si', "bo'lishi", 'kerak', 'emas', '.']
Pred: ['because', 'the', 'world', 'is', 'not', 'been', 'courase', 'to', 'be', 'courage', '.']
Gt: [['because', 'the', 'world', "'s", 'problems', 'shouldn', "'t", 'be', 'the', 'human', 'family', "'s", 'heirloom', '.']]
1) -------------------------- Input: ['(', '<UNK>', ')']
Pred: ['(', '<UNK>', ')']
Gt: [['(', 'applause', ')']]
2) -------------------------- Input: ['li@@', 'der@@', 'lik', ',', 'boshqar@@', 'ish', 'ker@@', 'a@@', 'gi@@', 'dan', 'or@@', 'ti@@', 'q@@', 'cha', 'maq@@', 'ta@@', 'ladi', ',', 'ana', 'u', "ko'@@", 'y@@', 'lak@@', 'siz', 'kishi', 'birinchi', 'bosh@@', 'ladi', 'va', 'hamma', 'sh@@', 'ara@@', 'f', 'unga', 'te@@', 'ga@@', 'di', ',', 'lekin', 'aslida', 'uni', 'li@@', 'der@@', 'ga', 'aylan@@', 'tir@@', 'gan', 'odam', 'ana', 'shu', 'birinchi', "qo'sh@@", 'ilgan', 'odam', "bo'ladi", '.']
Pred: ['the', 'first', 'thing', 'that', 'when', 'he', 'west', 'for', 'the', 'first', 'time', ',', 'and', 'he', 'wrote', 'back', 'to', 'the', 'first', 'time', ',', 'and', 'in', 'the', 'first', 'time', ',', 'but', 'the', 'answer', 'is', 'that', 'the', 'first', 'followers', ',', 'and', 'in', 'the', 'first', 'place', 'for', 'the', 'world', ',', 'but', 'you', 'know', 'what', "'s", 'just', 'seeing', 'for', 'the', 'first', 'place', '.']
Gt: [['yes', ',', 'it', 'was', 'the', 'shirtless', 'guy', 'who', 'was', 'first', ',', 'and', 'he', "'ll", 'get', 'all', 'the', 'credit', ',', 'but', 'it', 'was', 'really', 'the', 'first', 'follower', 'that', 'transformed', 'the', 'lone', 'nut', 'into', 'a', 'leader', '.']]
3) -------------------------- Input: ['u', 'yana', 'bir', 'yil', 'dar@@', 's@@', 'lar', 'olib', 'mash@@', 'q', 'qiladi', ',', 'va', 'endi', 'u', "to'@@", 'q@@', 'qi@@', 'z', 'yosh@@', 'da', '.']
Pred: ['it', "'s", 'the', 'wrong', 'with', 'the', 'same', 'time', ',', 'and', 'he', "'s", 'the', 'same', 'wards', '.']
Gt: [['he', 'practices', 'for', 'another', 'year', 'and', 'takes', 'lessons', '--', 'he', "'s", 'nine', '.']]
SacreBLEU: BLEU = 19.57 37.7/19.4/16.3/15.4 (BP = 0.945 ratio = 0.947 hyp_len = 3616 ref_len = 3819)
Test Bleu: 19.57 | lr: 0.0012 | Elapsed: 11.9412 min
Epoch: 230 Train loss: 0.9632 acc: 0.7644 lr: 0.0012 | Elapsed: 11.9765 min
Epoch: 231 Train loss: 0.9627 acc: 0.7661 lr: 0.0012 | Elapsed: 12.0138 min
Epoch: 232 Train loss: 0.9538 acc: 0.768 lr: 0.0012 | Elapsed: 12.0516 min
Epoch: 233 Train loss: 0.9566 acc: 0.7666 lr: 0.0012 | Elapsed: 12.089 min
Epoch: 234 Train loss: 0.9514 acc: 0.7693 lr: 0.0012 | Elapsed: 12.1268 min
Epoch: 235 Train loss: 0.9497 acc: 0.768 lr: 0.0012 | Elapsed: 12.1645 min
Epoch: 236 Train loss: 0.9484 acc: 0.7696 lr: 0.0012 | Elapsed: 12.2022 min
Epoch: 237 Train loss: 0.9562 acc: 0.7671 lr: 0.0012 | Elapsed: 12.2396 min
Epoch: 238 Train loss: 0.951 acc: 0.7683 lr: 0.0012 | Elapsed: 12.2775 min
Epoch: 239 Train loss: 0.9479 acc: 0.768
Val loss: 4.1825 acc: 0.4421 | Evaluation on Test Set
Evaluation Phase over 193 sentences-----------------------
0) -------------------------- Input: ['chunki', 'dunyo', 'muammo@@', 'lari', 'inson@@', 'larning', 'o@@', 'ila@@', 'viy', 'mer@@', 'o@@', 'si', "bo'lishi", 'kerak', 'emas', '.']
Pred: ['analyzaged', 'by', 'the', 'universe', '.']
Gt: [['because', 'the', 'world', "'s", 'problems', 'shouldn', "'t", 'be', 'the', 'human', 'family', "'s", 'heirloom', '.']]
1) -------------------------- Input: ['(', '<UNK>', ')']
Pred: ['(', 'laughter', ')']
Gt: [['(', 'applause', ')']]
2) -------------------------- Input: ['li@@', 'der@@', 'lik', ',', 'boshqar@@', 'ish', 'ker@@', 'a@@', 'gi@@', 'dan', 'or@@', 'ti@@', 'q@@', 'cha', 'maq@@', 'ta@@', 'ladi', ',', 'ana', 'u', "ko'@@", 'y@@', 'lak@@', 'siz', 'kishi', 'birinchi', 'bosh@@', 'ladi', 'va', 'hamma', 'sh@@', 'ara@@', 'f', 'unga', 'te@@', 'ga@@', 'di', ',', 'lekin', 'aslida', 'uni', 'li@@', 'der@@', 'ga', 'aylan@@', 'tir@@', 'gan', 'odam', 'ana', 'shu', 'birinchi', "qo'sh@@", 'ilgan', 'odam', "bo'ladi", '.']
Pred: ['wrote', 'this', 'plain', 'the', 'line', ',', 'in', 'the', 'west', 'wall', 'the', 'win', 'this', 'particular', 'keeping', 'up', 'to', 'the', 'first', 'generation', ',', 'but', 'the', 'first', 'generation', 'again', '.']
Gt: [['yes', ',', 'it', 'was', 'the', 'shirtless', 'guy', 'who', 'was', 'first', ',', 'and', 'he', "'ll", 'get', 'all', 'the', 'credit', ',', 'but', 'it', 'was', 'really', 'the', 'first', 'follower', 'that', 'transformed', 'the', 'lone', 'nut', 'into', 'a', 'leader', '.']]
3) -------------------------- Input: ['u', 'yana', 'bir', 'yil', 'dar@@', 's@@', 'lar', 'olib', 'mash@@', 'q', 'qiladi', ',', 'va', 'endi', 'u', "to'@@", 'q@@', 'qi@@', 'z', 'yosh@@', 'da', '.']
Pred: ['it', "'s", 'the', 'wrong', 'chord', 'when', 'he', "'s", 'the', 'wrong', 'chord', 'with', 'it', "'s", 'the', 'same', 'time', ',', 'and', 'she', "'s", 'the', 'same', 'wards', '.']
Gt: [['he', 'practices', 'for', 'another', 'year', 'and', 'takes', 'lessons', '--', 'he', "'s", 'nine', '.']]
SacreBLEU: BLEU = 19.52 38.2/19.6/16.8/16.1 (BP = 0.921 ratio = 0.924 hyp_len = 3528 ref_len = 3819)
Test Bleu: 19.52 | lr: 0.0012 | Elapsed: 12.4574 min
Epoch: 240 Train loss: 0.9419 acc: 0.7713 lr: 0.0012 | Elapsed: 12.4911 min
Epoch: 241 Train loss: 0.9428 acc: 0.7707 lr: 0.0012 | Elapsed: 12.5287 min
Epoch: 242 Train loss: 0.9373 acc: 0.7723 lr: 0.0012 | Elapsed: 12.5669 min
Epoch: 243 Train loss: 0.9414 acc: 0.7699 lr: 0.0012 | Elapsed: 12.6039 min
Epoch: 244 Train loss: 0.9366 acc: 0.7741 lr: 0.0012 | Elapsed: 12.6412 min
Epoch: 245 Train loss: 0.9377 acc: 0.7731 lr: 0.0012 | Elapsed: 12.6789 min
Epoch: 246 Train loss: 0.9374 acc: 0.7713 lr: 0.0012 | Elapsed: 12.716 min
Epoch: 247 Train loss: 0.9354 acc: 0.7722 lr: 0.0012 | Elapsed: 12.7535 min
Epoch: 248 Train loss: 0.9309 acc: 0.7762 lr: 0.0012 | Elapsed: 12.7914 min
Epoch: 249 Train loss: 0.9373 acc: 0.7722
Val loss: 4.1806 acc: 0.4399 | Evaluation on Test Set
Evaluation Phase over 193 sentences-----------------------
0) -------------------------- Input: ['chunki', 'dunyo', 'muammo@@', 'lari', 'inson@@', 'larning', 'o@@', 'ila@@', 'viy', 'mer@@', 'o@@', 'si', "bo'lishi", 'kerak', 'emas', '.']
Pred: ['because', 'the', 'universe', 'of', 'people', '.']
Gt: [['because', 'the', 'world', "'s", 'problems', 'shouldn', "'t", 'be', 'the', 'human', 'family', "'s", 'heirloom', '.']]
1) -------------------------- Input: ['(', '<UNK>', ')']
Pred: ['(', '<UNK>', ')']
Gt: [['(', 'applause', ')']]
2) -------------------------- Input: ['li@@', 'der@@', 'lik', ',', 'boshqar@@', 'ish', 'ker@@', 'a@@', 'gi@@', 'dan', 'or@@', 'ti@@', 'q@@', 'cha', 'maq@@', 'ta@@', 'ladi', ',', 'ana', 'u', "ko'@@", 'y@@', 'lak@@', 'siz', 'kishi', 'birinchi', 'bosh@@', 'ladi', 'va', 'hamma', 'sh@@', 'ara@@', 'f', 'unga', 'te@@', 'ga@@', 'di', ',', 'lekin', 'aslida', 'uni', 'li@@', 'der@@', 'ga', 'aylan@@', 'tir@@', 'gan', 'odam', 'ana', 'shu', 'birinchi', "qo'sh@@", 'ilgan', 'odam', "bo'ladi", '.']
Pred: ['whether', 'patterns', ',', 'he', 'wall', 'the', 'first', 'generation', ',', 'he', "'s", 'going', 'to', 'be', 'able', 'to', 'machine', '!']
Gt: [['yes', ',', 'it', 'was', 'the', 'shirtless', 'guy', 'who', 'was', 'first', ',', 'and', 'he', "'ll", 'get', 'all', 'the', 'credit', ',', 'but', 'it', 'was', 'really', 'the', 'first', 'follower', 'that', 'transformed', 'the', 'lone', 'nut', 'into', 'a', 'leader', '.']]
3) -------------------------- Input: ['u', 'yana', 'bir', 'yil', 'dar@@', 's@@', 'lar', 'olib', 'mash@@', 'q', 'qiladi', ',', 'va', 'endi', 'u', "to'@@", 'q@@', 'qi@@', 'z', 'yosh@@', 'da', '.']
Pred: ['it', "'s", 'the', 'wrong', 'chord', 'when', 'he', "'s", 'the', 'wrong', 'with', 'the', 'same', 'time', ',', 'and', 'he', "'s", 'the', 'same', 'wards', '.']
Gt: [['he', 'practices', 'for', 'another', 'year', 'and', 'takes', 'lessons', '--', 'he', "'s", 'nine', '.']]
SacreBLEU: BLEU = 19.47 38.9/20.4/17.3/16.4 (BP = 0.894 ratio = 0.899 hyp_len = 3433 ref_len = 3819)
Test Bleu: 19.47 | lr: 0.0012 | Elapsed: 12.9461 min
Epoch: 250 Train loss: 0.9246 acc: 0.7757 lr: 0.0012 | Elapsed: 12.9839 min
Epoch: 251 Train loss: 0.9317 acc: 0.7753 lr: 0.0012 | Elapsed: 13.0236 min
Epoch: 252 Train loss: 0.931 acc: 0.7753 lr: 0.0012 | Elapsed: 13.0614 min
Epoch: 253 Train loss: 0.9295 acc: 0.7747 lr: 0.0012 | Elapsed: 13.0991 min
Epoch: 254 Train loss: 0.9303 acc: 0.7739 lr: 0.0012 | Elapsed: 13.1372 min
Epoch: 255 Train loss: 0.9188 acc: 0.7783 lr: 0.0012 | Elapsed: 13.1744 min
Epoch: 256 Train loss: 0.9207 acc: 0.7784 lr: 0.0012 | Elapsed: 13.2114 min
Epoch: 257 Train loss: 0.9185 acc: 0.778 lr: 0.0012 | Elapsed: 13.2495 min
Epoch: 258 Train loss: 0.9189 acc: 0.78 lr: 0.0012 | Elapsed: 13.2875 min
Epoch: 259 Train loss: 0.9229 acc: 0.7771
Val loss: 4.1765 acc: 0.4396 | Evaluation on Test Set
Evaluation Phase over 193 sentences-----------------------
0) -------------------------- Input: ['chunki', 'dunyo', 'muammo@@', 'lari', 'inson@@', 'larning', 'o@@', 'ila@@', 'viy', 'mer@@', 'o@@', 'si', "bo'lishi", 'kerak', 'emas', '.']
Pred: ['because', 'the', 'impact', 'is', 'not', 'to', 'be', 'honor', '.']
Gt: [['because', 'the', 'world', "'s", 'problems', 'shouldn', "'t", 'be', 'the', 'human', 'family', "'s", 'heirloom', '.']]
1) -------------------------- Input: ['(', '<UNK>', ')']
Pred: ['(', 'laughter', ')']
Gt: [['(', 'applause', ')']]
2) -------------------------- Input: ['li@@', 'der@@', 'lik', ',', 'boshqar@@', 'ish', 'ker@@', 'a@@', 'gi@@', 'dan', 'or@@', 'ti@@', 'q@@', 'cha', 'maq@@', 'ta@@', 'ladi', ',', 'ana', 'u', "ko'@@", 'y@@', 'lak@@', 'siz', 'kishi', 'birinchi', 'bosh@@', 'ladi', 'va', 'hamma', 'sh@@', 'ara@@', 'f', 'unga', 'te@@', 'ga@@', 'di', ',', 'lekin', 'aslida', 'uni', 'li@@', 'der@@', 'ga', 'aylan@@', 'tir@@', 'gan', 'odam', 'ana', 'shu', 'birinchi', "qo'sh@@", 'ilgan', 'odam', "bo'ladi", '.']
Pred: ['where', 'comes', 'the', 'first', 'thing', ',', 'when', 'he', 'wall', 'the', 'first', 'generation', ',', 'the', 'first', 'generation', ',', 'the', 'world', 'will', 'be', 'for', 'the', 'first', 'generation', 'in', 'the', 'first', 'generation', '.']
Gt: [['yes', ',', 'it', 'was', 'the', 'shirtless', 'guy', 'who', 'was', 'first', ',', 'and', 'he', "'ll", 'get', 'all', 'the', 'credit', ',', 'but', 'it', 'was', 'really', 'the', 'first', 'follower', 'that', 'transformed', 'the', 'lone', 'nut', 'into', 'a', 'leader', '.']]
3) -------------------------- Input: ['u', 'yana', 'bir', 'yil', 'dar@@', 's@@', 'lar', 'olib', 'mash@@', 'q', 'qiladi', ',', 'va', 'endi', 'u', "to'@@", 'q@@', 'qi@@', 'z', 'yosh@@', 'da', '.']
Pred: ['he', 'practices', 'for', 'another', 'year', 'and', 'takes', 'lessons', '--', 'now', 'he', "'s", '10', '.']
Gt: [['he', 'practices', 'for', 'another', 'year', 'and', 'takes', 'lessons', '--', 'he', "'s", 'nine', '.']]
SacreBLEU: BLEU = 19.69 38.9/21.1/17.9/17.0 (BP = 0.881 ratio = 0.888 hyp_len = 3391 ref_len = 3819)
Test Bleu: 19.69 | lr: 0.0012 | Elapsed: 13.4546 min
Epoch: 260 Train loss: 0.9112 acc: 0.7824 lr: 0.0012 | Elapsed: 13.4895 min
Epoch: 261 Train loss: 0.9173 acc: 0.7788 lr: 0.0012 | Elapsed: 13.5267 min
Epoch: 262 Train loss: 0.9092 acc: 0.7824 lr: 0.0012 | Elapsed: 13.5643 min
Epoch: 263 Train loss: 0.9119 acc: 0.781 lr: 0.0012 | Elapsed: 13.6027 min
Epoch: 264 Train loss: 0.9119 acc: 0.7806 lr: 0.0012 | Elapsed: 13.6406 min
Epoch: 265 Train loss: 0.908 acc: 0.7819 lr: 0.0012 | Elapsed: 13.6781 min
Epoch: 266 Train loss: 0.9056 acc: 0.7829 lr: 0.0012 | Elapsed: 13.716 min
Epoch: 267 Train loss: 0.9067 acc: 0.7821 lr: 0.0012 | Elapsed: 13.754 min
Epoch: 268 Train loss: 0.9015 acc: 0.7832 lr: 0.0011 | Elapsed: 13.7918 min
Epoch: 269 Train loss: 0.9051 acc: 0.7841
Val loss: 4.1932 acc: 0.4339 | Evaluation on Test Set
Evaluation Phase over 193 sentences-----------------------
0) -------------------------- Input: ['chunki', 'dunyo', 'muammo@@', 'lari', 'inson@@', 'larning', 'o@@', 'ila@@', 'viy', 'mer@@', 'o@@', 'si', "bo'lishi", 'kerak', 'emas', '.']
Pred: ['because', 'the', 'impact', 'is', 'not', 'to', 'be', 'honor', 'of', 'people', '.']
Gt: [['because', 'the', 'world', "'s", 'problems', 'shouldn', "'t", 'be', 'the', 'human', 'family', "'s", 'heirloom', '.']]
1) -------------------------- Input: ['(', '<UNK>', ')']
Pred: ['(', 'laughter', ')']
Gt: [['(', 'applause', ')']]
2) -------------------------- Input: ['li@@', 'der@@', 'lik', ',', 'boshqar@@', 'ish', 'ker@@', 'a@@', 'gi@@', 'dan', 'or@@', 'ti@@', 'q@@', 'cha', 'maq@@', 'ta@@', 'ladi', ',', 'ana', 'u', "ko'@@", 'y@@', 'lak@@', 'siz', 'kishi', 'birinchi', 'bosh@@', 'ladi', 'va', 'hamma', 'sh@@', 'ara@@', 'f', 'unga', 'te@@', 'ga@@', 'di', ',', 'lekin', 'aslida', 'uni', 'li@@', 'der@@', 'ga', 'aylan@@', 'tir@@', 'gan', 'odam', 'ana', 'shu', 'birinchi', "qo'sh@@", 'ilgan', 'odam', "bo'ladi", '.']
Pred: ['west', 'this', 'was', 'invented', 'in', 'the', 'first', 'time', ',', 'in', 'this', 'particular', 'journey', ',', 'when', 'he', 'wine', ',', 'and', 'the', 'first', 'international', 'with', 'the', 'first', 'place', 'it', 'in', 'the', 'first', 'life', '.']
Gt: [['yes', ',', 'it', 'was', 'the', 'shirtless', 'guy', 'who', 'was', 'first', ',', 'and', 'he', "'ll", 'get', 'all', 'the', 'credit', ',', 'but', 'it', 'was', 'really', 'the', 'first', 'follower', 'that', 'transformed', 'the', 'lone', 'nut', 'into', 'a', 'leader', '.']]
3) -------------------------- Input: ['u', 'yana', 'bir', 'yil', 'dar@@', 's@@', 'lar', 'olib', 'mash@@', 'q', 'qiladi', ',', 'va', 'endi', 'u', "to'@@", 'q@@', 'qi@@', 'z', 'yosh@@', 'da', '.']
Pred: ['it', "'s", 'the', 'wrong', 'with', 'the', 'same', 'time', ',', 'and', 'he', "'s", 'the', 'wrong', 'with', 'the', 'same', 'time', '.']
Gt: [['he', 'practices', 'for', 'another', 'year', 'and', 'takes', 'lessons', '--', 'he', "'s", 'nine', '.']]
SacreBLEU: BLEU = 20.04 37.7/19.4/16.2/15.3 (BP = 0.971 ratio = 0.971 hyp_len = 3709 ref_len = 3819)
Test Bleu: 20.04 | lr: 0.0011 | Elapsed: 13.9485 min
Epoch: 270 Train loss: 0.8958 acc: 0.7878 lr: 0.0011 | Elapsed: 13.9863 min
Epoch: 271 Train loss: 0.9032 acc: 0.7825 lr: 0.0011 | Elapsed: 14.0223 min
Epoch: 272 Train loss: 0.9011 acc: 0.7854 lr: 0.0011 | Elapsed: 14.0591 min
Epoch: 273 Train loss: 0.8989 acc: 0.7856 lr: 0.0011 | Elapsed: 14.0969 min
Epoch: 274 Train loss: 0.8985 acc: 0.7862lr: 0.0011 | Elapsed: 14.1345 min
Epoch: 275 Train loss: 0.893 acc: 0.7876 lr: 0.0011 | Elapsed: 14.1721 min
Epoch: 276 Train loss: 0.8969 acc: 0.786 lr: 0.0011 | Elapsed: 14.2096 min
Epoch: 277 Train loss: 0.9013 acc: 0.7845 lr: 0.0011 | Elapsed: 14.2437 min
Epoch: 278 Train loss: 0.8931 acc: 0.7855 lr: 0.0011 | Elapsed: 14.2805 min
Epoch: 279 Train loss: 0.8926 acc: 0.788
Val loss: 4.2093 acc: 0.4411 | Evaluation on Test Set
Evaluation Phase over 193 sentences-----------------------
0) -------------------------- Input: ['chunki', 'dunyo', 'muammo@@', 'lari', 'inson@@', 'larning', 'o@@', 'ila@@', 'viy', 'mer@@', 'o@@', 'si', "bo'lishi", 'kerak', 'emas', '.']
Pred: ['because', 'the', 'imple', 'to', 'be', 'verse', '.']
Gt: [['because', 'the', 'world', "'s", 'problems', 'shouldn', "'t", 'be', 'the', 'human', 'family', "'s", 'heirloom', '.']]
1) -------------------------- Input: ['(', '<UNK>', ')']
Pred: ['(', 'laughter', ')']
Gt: [['(', 'applause', ')']]
2) -------------------------- Input: ['li@@', 'der@@', 'lik', ',', 'boshqar@@', 'ish', 'ker@@', 'a@@', 'gi@@', 'dan', 'or@@', 'ti@@', 'q@@', 'cha', 'maq@@', 'ta@@', 'ladi', ',', 'ana', 'u', "ko'@@", 'y@@', 'lak@@', 'siz', 'kishi', 'birinchi', 'bosh@@', 'ladi', 'va', 'hamma', 'sh@@', 'ara@@', 'f', 'unga', 'te@@', 'ga@@', 'di', ',', 'lekin', 'aslida', 'uni', 'li@@', 'der@@', 'ga', 'aylan@@', 'tir@@', 'gan', 'odam', 'ana', 'shu', 'birinchi', "qo'sh@@", 'ilgan', 'odam', "bo'ladi", '.']
Pred: ['whether', 'again', ',', 'he', "'s", 'just', 'in', 'this', 'last', 'thing', ',', 'when', 'he', 'west', 'thinking', 'in', 'the', 'last', '30', 'days', ',', 'and', 'the', 'first', 'interplain', 'the', 'first', 'generation', ',', 'but', 'everything', 'that', 'made', 'it', 'in', 'their', 'own', 'hands', '.']
Gt: [['yes', ',', 'it', 'was', 'the', 'shirtless', 'guy', 'who', 'was', 'first', ',', 'and', 'he', "'ll", 'get', 'all', 'the', 'credit', ',', 'but', 'it', 'was', 'really', 'the', 'first', 'follower', 'that', 'transformed', 'the', 'lone', 'nut', 'into', 'a', 'leader', '.']]
3) -------------------------- Input: ['u', 'yana', 'bir', 'yil', 'dar@@', 's@@', 'lar', 'olib', 'mash@@', 'q', 'qiladi', ',', 'va', 'endi', 'u', "to'@@", 'q@@', 'qi@@', 'z', 'yosh@@', 'da', '.']
Pred: ['it', "'s", 'the', 'wrong', 'with', 'the', 'same', 'time', ',', 'and', 'he', "'s", 'the', 'wrong', '.']
Gt: [['he', 'practices', 'for', 'another', 'year', 'and', 'takes', 'lessons', '--', 'he', "'s", 'nine', '.']]
SacreBLEU: BLEU = 20.01 39.2/19.8/16.6/15.7 (BP = 0.945 ratio = 0.946 hyp_len = 3613 ref_len = 3819)
Test Bleu: 20.01 | lr: 0.0011 | Elapsed: 14.4333 min
Epoch: 280 Train loss: 0.8908 acc: 0.7876
lr: 0.0011 | Elapsed: 14.472 min
Epoch: 281 Train loss: 0.8897 acc: 0.7889 lr: 0.0011 | Elapsed: 14.5092 min
Epoch: 282 Train loss: 0.888 acc: 0.7885 lr: 0.0011 | Elapsed: 14.5469 min
Epoch: 283 Train loss: 0.8898 acc: 0.7878 lr: 0.0011 | Elapsed: 14.5848 min
Epoch: 284 Train loss: 0.8946 acc: 0.7859 lr: 0.0011 | Elapsed: 14.6222 min
Epoch: 285 Train loss: 0.8946 acc: 0.7869 lr: 0.0011 | Elapsed: 14.66 min
Epoch: 286 Train loss: 0.8855 acc: 0.7896 lr: 0.0011 | Elapsed: 14.6962 min
Epoch: 287 Train loss: 0.8864 acc: 0.7887 lr: 0.0011 | Elapsed: 14.7341 min
Epoch: 288 Train loss: 0.8872 acc: 0.7897 lr: 0.0011 | Elapsed: 14.7722 min
Epoch: 289 Train loss: 0.8844 acc: 0.7904
Val loss: 4.1979 acc: 0.4295 | Evaluation on Test Set
Evaluation Phase over 193 sentences-----------------------
0) -------------------------- Input: ['chunki', 'dunyo', 'muammo@@', 'lari', 'inson@@', 'larning', 'o@@', 'ila@@', 'viy', 'mer@@', 'o@@', 'si', "bo'lishi", 'kerak', 'emas', '.']
Pred: ['because', 'the', 'impact', 'of', 'the', 'world', "'s", 'not', 'true', '.']
Gt: [['because', 'the', 'world', "'s", 'problems', 'shouldn', "'t", 'be', 'the', 'human', 'family', "'s", 'heirloom', '.']]
1) -------------------------- Input: ['(', '<UNK>', ')']
Pred: ['(', '<UNK>', ')']
Gt: [['(', 'applause', ')']]
2) -------------------------- Input: ['li@@', 'der@@', 'lik', ',', 'boshqar@@', 'ish', 'ker@@', 'a@@', 'gi@@', 'dan', 'or@@', 'ti@@', 'q@@', 'cha', 'maq@@', 'ta@@', 'ladi', ',', 'ana', 'u', "ko'@@", 'y@@', 'lak@@', 'siz', 'kishi', 'birinchi', 'bosh@@', 'ladi', 'va', 'hamma', 'sh@@', 'ara@@', 'f', 'unga', 'te@@', 'ga@@', 'di', ',', 'lekin', 'aslida', 'uni', 'li@@', 'der@@', 'ga', 'aylan@@', 'tir@@', 'gan', 'odam', 'ana', 'shu', 'birinchi', "qo'sh@@", 'ilgan', 'odam', "bo'ladi", '.']
Pred: ['where', 'again', ',', 'he', 'win', 'this', 'last', '30', 'days', 'when', 'he', 'win', 'the', 'first', 'time', ',', 'the', 'first', 'time', ',', 'the', 'first', 'generation', ',', 'to', 'be', 'the', 'first', 'generation', ',', 'to', 'be', 'able', 'to', 'be', 'plain', 'the', 'first', '.']
Gt: [['yes', ',', 'it', 'was', 'the', 'shirtless', 'guy', 'who', 'was', 'first', ',', 'and', 'he', "'ll", 'get', 'all', 'the', 'credit', ',', 'but', 'it', 'was', 'really', 'the', 'first', 'follower', 'that', 'transformed', 'the', 'lone', 'nut', 'into', 'a', 'leader', '.']]
3) -------------------------- Input: ['u', 'yana', 'bir', 'yil', 'dar@@', 's@@', 'lar', 'olib', 'mash@@', 'q', 'qiladi', ',', 'va', 'endi', 'u', "to'@@", 'q@@', 'qi@@', 'z', 'yosh@@', 'da', '.']
Pred: ['it', "'s", 'the', 'wrong', 'chord', 'when', 'he', "'s", 'the', 'wrong', 'chord', '.']
Gt: [['he', 'practices', 'for', 'another', 'year', 'and', 'takes', 'lessons', '--', 'he', "'s", 'nine', '.']]
SacreBLEU: BLEU = 19.83 39.2/20.3/17.1/16.3 (BP = 0.913 ratio = 0.917 hyp_len = 3501 ref_len = 3819)
Test Bleu: 19.83 | lr: 0.0011 | Elapsed: 14.9216 min
Epoch: 290 Train loss: 0.8849 acc: 0.7884 lr: 0.0011 | Elapsed: 14.9583 min
Epoch: 291 Train loss: 0.8816 acc: 0.7907 lr: 0.0011 | Elapsed: 14.9953 min
Epoch: 292 Train loss: 0.8791 acc: 0.7925 lr: 0.0011 | Elapsed: 15.0304 min
Epoch: 293 Train loss: 0.8778 acc: 0.7921 lr: 0.0011 | Elapsed: 15.0643 min
Epoch: 294 Train loss: 0.8779 acc: 0.7935 lr: 0.0011 | Elapsed: 15.1019 min
Epoch: 295 Train loss: 0.8758 acc: 0.7914 lr: 0.0011 | Elapsed: 15.1375 min
Epoch: 296 Train loss: 0.8753 acc: 0.7908 lr: 0.0011 | Elapsed: 15.1714 min
Epoch: 297 Train loss: 0.8714 acc: 0.7943 lr: 0.0011 | Elapsed: 15.2072 min
Epoch: 298 Train loss: 0.8723 acc: 0.7933 lr: 0.0011 | Elapsed: 15.2451 min
Epoch: 299 Train loss: 0.8717 acc: 0.7935
Val loss: 4.2019 acc: 0.4369 | Evaluation on Test Set
Evaluation Phase over 193 sentences-----------------------
0) -------------------------- Input: ['chunki', 'dunyo', 'muammo@@', 'lari', 'inson@@', 'larning', 'o@@', 'ila@@', 'viy', 'mer@@', 'o@@', 'si', "bo'lishi", 'kerak', 'emas', '.']
Pred: ['analyway', 'people', 'had', 'to', 'be', 'otherwise', 'the', 'play', 'eries', '.']
Gt: [['because', 'the', 'world', "'s", 'problems', 'shouldn', "'t", 'be', 'the', 'human', 'family', "'s", 'heirloom', '.']]
1) -------------------------- Input: ['(', '<UNK>', ')']
Pred: ['(', '<UNK>', ')']
Gt: [['(', 'applause', ')']]
2) -------------------------- Input: ['li@@', 'der@@', 'lik', ',', 'boshqar@@', 'ish', 'ker@@', 'a@@', 'gi@@', 'dan', 'or@@', 'ti@@', 'q@@', 'cha', 'maq@@', 'ta@@', 'ladi', ',', 'ana', 'u', "ko'@@", 'y@@', 'lak@@', 'siz', 'kishi', 'birinchi', 'bosh@@', 'ladi', 'va', 'hamma', 'sh@@', 'ara@@', 'f', 'unga', 'te@@', 'ga@@', 'di', ',', 'lekin', 'aslida', 'uni', 'li@@', 'der@@', 'ga', 'aylan@@', 'tir@@', 'gan', 'odam', 'ana', 'shu', 'birinchi', "qo'sh@@", 'ilgan', 'odam', "bo'ladi", '.']
Pred: ['he', 'wrote', 'this', 'to', 'be', 'the', 'first', 'time', ',', 'in', 'the', 'west', 'villawin', 'this', 'particular', 'keeping', 'up', 'to', 'the', 'first', 'time', ',', 'but', 'the', 'first', 'generation', 'again', '.']
Gt: [['yes', ',', 'it', 'was', 'the', 'shirtless', 'guy', 'who', 'was', 'first', ',', 'and', 'he', "'ll", 'get', 'all', 'the', 'credit', ',', 'but', 'it', 'was', 'really', 'the', 'first', 'follower', 'that', 'transformed', 'the', 'lone', 'nut', 'into', 'a', 'leader', '.']]
3) -------------------------- Input: ['u', 'yana', 'bir', 'yil', 'dar@@', 's@@', 'lar', 'olib', 'mash@@', 'q', 'qiladi', ',', 'va', 'endi', 'u', "to'@@", 'q@@', 'qi@@', 'z', 'yosh@@', 'da', '.']
Pred: ['it', "'s", 'the', 'wrong', 'with', 'the', 'same', 'time', ',', 'and', 'he', "'s", 'the', 'wrong', 'with', 'the', 'same', 'patterwards', '.']
Gt: [['he', 'practices', 'for', 'another', 'year', 'and', 'takes', 'lessons', '--', 'he', "'s", 'nine', '.']]
SacreBLEU: BLEU = 19.83 38.6/19.9/16.8/15.9 (BP = 0.932 ratio = 0.934 hyp_len = 3567 ref_len = 3819)
Test Bleu: 19.83 | lr: 0.0011 | Elapsed: 15.3977 min
Epoch: 300 Train loss: 0.8683 acc: 0.7949 lr: 0.0011 | Elapsed: 15.4351 min
Epoch: 301 Train loss: 0.8707 acc: 0.7949 lr: 0.0011 | Elapsed: 15.4734 min
Epoch: 302 Train loss: 0.8695 acc: 0.7935 lr: 0.0011 | Elapsed: 15.5094 min
Epoch: 303 Train loss: 0.8715 acc: 0.793 lr: 0.0011 | Elapsed: 15.5474 min
Epoch: 304 Train loss: 0.8693 acc: 0.794 lr: 0.0011 | Elapsed: 15.5848 min
Epoch: 305 Train loss: 0.8643 acc: 0.7949 lr: 0.0011 | Elapsed: 15.6228 min
Epoch: 306 Train loss: 0.8636 acc: 0.7964 lr: 0.0011 | Elapsed: 15.6603 min
Epoch: 307 Train loss: 0.861 acc: 0.7965 lr: 0.0011 | Elapsed: 15.6979 min
Epoch: 308 Train loss: 0.8617 acc: 0.7981 lr: 0.0011 | Elapsed: 15.7357 min
Epoch: 309 Train loss: 0.8617 acc: 0.7974
Val loss: 4.2176 acc: 0.435 | Evaluation on Test Set
Evaluation Phase over 193 sentences-----------------------
0) -------------------------- Input: ['chunki', 'dunyo', 'muammo@@', 'lari', 'inson@@', 'larning', 'o@@', 'ila@@', 'viy', 'mer@@', 'o@@', 'si', "bo'lishi", 'kerak', 'emas', '.']
Pred: ['because', 'the', 'impact', 'of', 'the', 'world', 'is', 'not', 'to', 'make', 'the', 'problem', '.']
Gt: [['because', 'the', 'world', "'s", 'problems', 'shouldn', "'t", 'be', 'the', 'human', 'family', "'s", 'heirloom', '.']]
1) -------------------------- Input: ['(', '<UNK>', ')']
Pred: ['(', '<UNK>', ')']
Gt: [['(', 'applause', ')']]
2) -------------------------- Input: ['li@@', 'der@@', 'lik', ',', 'boshqar@@', 'ish', 'ker@@', 'a@@', 'gi@@', 'dan', 'or@@', 'ti@@', 'q@@', 'cha', 'maq@@', 'ta@@', 'ladi', ',', 'ana', 'u', "ko'@@", 'y@@', 'lak@@', 'siz', 'kishi', 'birinchi', 'bosh@@', 'ladi', 'va', 'hamma', 'sh@@', 'ara@@', 'f', 'unga', 'te@@', 'ga@@', 'di', ',', 'lekin', 'aslida', 'uni', 'li@@', 'der@@', 'ga', 'aylan@@', 'tir@@', 'gan', 'odam', 'ana', 'shu', 'birinchi', "qo'sh@@", 'ilgan', 'odam', "bo'ladi", '.']
Pred: ['whether', 'again', ',', 'he', 'win', 'this', 'last', 'box', 'that', 'made', 'it', 'in', 'box', 'for', '20', 'years', ',', 'when', 'the', 'first', 'generate', '.']
Gt: [['yes', ',', 'it', 'was', 'the', 'shirtless', 'guy', 'who', 'was', 'first', ',', 'and', 'he', "'ll", 'get', 'all', 'the', 'credit', ',', 'but', 'it', 'was', 'really', 'the', 'first', 'follower', 'that', 'transformed', 'the', 'lone', 'nut', 'into', 'a', 'leader', '.']]
3) -------------------------- Input: ['u', 'yana', 'bir', 'yil', 'dar@@', 's@@', 'lar', 'olib', 'mash@@', 'q', 'qiladi', ',', 'va', 'endi', 'u', "to'@@", 'q@@', 'qi@@', 'z', 'yosh@@', 'da', '.']
Pred: ['it', "'s", 'the', 'wrong', 'with', 'her', ',', 'and', 'he', "'s", 'the', 'wrong', 'with', 'the', 'same', 'pattern', '.']
Gt: [['he', 'practices', 'for', 'another', 'year', 'and', 'takes', 'lessons', '--', 'he', "'s", 'nine', '.']]
SacreBLEU: BLEU = 19.70 40.1/20.8/17.7/16.9 (BP = 0.882 ratio = 0.889 hyp_len = 3394 ref_len = 3819)
Test Bleu: 19.70 | lr: 0.0011 | Elapsed: 15.8855 min
Epoch: 310 Train loss: 0.8612 acc: 0.7962 lr: 0.0011 | Elapsed: 15.9249 min
Epoch: 311 Train loss: 0.8566 acc: 0.7978 lr: 0.0011 | Elapsed: 15.9625 min
Epoch: 312 Train loss: 0.8588 acc: 0.7994 lr: 0.0011 | Elapsed: 16.0001 min
Epoch: 313 Train loss: 0.8573 acc: 0.7989 lr: 0.0011 | Elapsed: 16.0381 min
Epoch: 314 Train loss: 0.8603 acc: 0.7994 lr: 0.0011 | Elapsed: 16.0755 min
Epoch: 315 Train loss: 0.8517 acc: 0.8014 lr: 0.0011 | Elapsed: 16.113 min
Epoch: 316 Train loss: 0.8517 acc: 0.8 lr: 0.0011 | Elapsed: 16.1504 min
Epoch: 317 Train loss: 0.8545 acc: 0.8003 lr: 0.0011 | Elapsed: 16.1884 min
Epoch: 318 Train loss: 0.8494 acc: 0.7995 lr: 0.0011 | Elapsed: 16.2262 min
Epoch: 319 Train loss: 0.8535 acc: 0.8012
Val loss: 4.1997 acc: 0.4358 | Evaluation on Test Set
Evaluation Phase over 193 sentences-----------------------
0) -------------------------- Input: ['chunki', 'dunyo', 'muammo@@', 'lari', 'inson@@', 'larning', 'o@@', 'ila@@', 'viy', 'mer@@', 'o@@', 'si', "bo'lishi", 'kerak', 'emas', '.']
Pred: ['because', 'the', 'world', 'would', 'be', 'ricientic', 'anymore', '.']
Gt: [['because', 'the', 'world', "'s", 'problems', 'shouldn', "'t", 'be', 'the', 'human', 'family', "'s", 'heirloom', '.']]
1) -------------------------- Input: ['(', '<UNK>', ')']
Pred: ['(', '<UNK>', ')']
Gt: [['(', 'applause', ')']]
2) -------------------------- Input: ['li@@', 'der@@', 'lik', ',', 'boshqar@@', 'ish', 'ker@@', 'a@@', 'gi@@', 'dan', 'or@@', 'ti@@', 'q@@', 'cha', 'maq@@', 'ta@@', 'ladi', ',', 'ana', 'u', "ko'@@", 'y@@', 'lak@@', 'siz', 'kishi', 'birinchi', 'bosh@@', 'ladi', 'va', 'hamma', 'sh@@', 'ara@@', 'f', 'unga', 'te@@', 'ga@@', 'di', ',', 'lekin', 'aslida', 'uni', 'li@@', 'der@@', 'ga', 'aylan@@', 'tir@@', 'gan', 'odam', 'ana', 'shu', 'birinchi', "qo'sh@@", 'ilgan', 'odam', "bo'ladi", '.']
Pred: ['wrote', 'this', 'plain', 'the', 'last', '30', 'days', ',', 'and', 'the', 'first', 'study', 'this', 'to', 'be', 'the', 'first', 'in', 'the', 'last', 'night', ',', 'when', 'you', 'look', 'old', 'the', 'first', '.']
Gt: [['yes', ',', 'it', 'was', 'the', 'shirtless', 'guy', 'who', 'was', 'first', ',', 'and', 'he', "'ll", 'get', 'all', 'the', 'credit', ',', 'but', 'it', 'was', 'really', 'the', 'first', 'follower', 'that', 'transformed', 'the', 'lone', 'nut', 'into', 'a', 'leader', '.']]
3) -------------------------- Input: ['u', 'yana', 'bir', 'yil', 'dar@@', 's@@', 'lar', 'olib', 'mash@@', 'q', 'qiladi', ',', 'va', 'endi', 'u', "to'@@", 'q@@', 'qi@@', 'z', 'yosh@@', 'da', '.']
Pred: ['it', "'s", 'the', 'wrong', 'chord', 'for', 'this', ',', 'and', 'she', "'s", 'the', 'same', 'time', 'for', 'mystay', 'wards', '.']
Gt: [['he', 'practices', 'for', 'another', 'year', 'and', 'takes', 'lessons', '--', 'he', "'s", 'nine', '.']]
SacreBLEU: BLEU = 19.77 39.2/20.2/17.1/16.3 (BP = 0.913 ratio = 0.916 hyp_len = 3500 ref_len = 3819)
Test Bleu: 19.77 | lr: 0.0011 | Elapsed: 16.3769 min
